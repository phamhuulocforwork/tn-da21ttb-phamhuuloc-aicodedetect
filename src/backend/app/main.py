import os
import sys
import traceback
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import tempfile
import json
import asyncio
import zipfile
try:
    import rarfile
    RARFILE_AVAILABLE = True
except ImportError:
    RARFILE_AVAILABLE = False
    class rarfile:
        @staticmethod
        def RarFile(*args, **kwargs):
            raise ImportError("rarfile module not available")

try:
    import googleapiclient.discovery
    from googleapiclient.http import MediaIoBaseDownload
    GOOGLE_DRIVE_AVAILABLE = True
except ImportError:
    GOOGLE_DRIVE_AVAILABLE = False
import shutil
import re
from urllib.parse import urlparse
import aiofiles
import aiohttp

from fastapi import FastAPI, HTTPException, UploadFile, File, Form, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator
import uvicorn
from dotenv import load_dotenv

load_dotenv()

try:
    from google import genai
    GENAI_AVAILABLE = True
except ImportError as e:
    GENAI_AVAILABLE = False

try:
    from baseline_loader import get_baseline_loader, reload_baseline_stats
    BASELINE_LOADER_AVAILABLE = True
except ImportError as e:
    BASELINE_LOADER_AVAILABLE = False

try:
    from chart_helpers import prepare_boxplot_data, prepare_enhanced_chart_data
    CHART_HELPERS_AVAILABLE = True
except ImportError as e:
    CHART_HELPERS_AVAILABLE = False
    
    def prepare_boxplot_data(feature_groups):
        return {}
    
    def prepare_enhanced_chart_data(feature_groups):
        return {"boxplot": {}, "comparison": {}, "distribution": {}, "correlation": {}}

if BASELINE_LOADER_AVAILABLE:
    try:
        test_loader = get_baseline_loader()
        test_stats = test_loader.get_feature_stats_summary()
    except Exception as e:
        BASELINE_LOADER_AVAILABLE = False

current_dir = Path(__file__).parent.absolute()
src_dir = current_dir.parent.parent / "src"
sys.path.insert(0, str(src_dir))

try:
    from features.advanced_features import AdvancedFeatureExtractor, ComprehensiveFeatures
    from features.ast_analyzer import CppASTAnalyzer, ASTFeatures
    from features.human_style_analyzer import HumanStyleAnalyzer, HumanStyleFeatures
    from features.detection_models import create_detector
    ANALYSIS_MODULES_AVAILABLE = True
except ImportError as e:
    ANALYSIS_MODULES_AVAILABLE = False
    class AdvancedFeatureExtractor:
        def extract_all_features(self, code: str, filename: str = "") -> Dict:
            return {"error": "Module phân tích không khả dụng"}
    
    class CppASTAnalyzer:
        def analyze_code(self, code: str, filename: str = "") -> Dict:
            return {"error": "AST analyzer không khả dụng"}
    
    class HumanStyleAnalyzer:
        def analyze_code(self, code: str, filename: str = "") -> Dict:
            return {"error": "Human style analyzer không khả dụng"}

app = FastAPI(
    title="API Phân tích phát hiện mã AI",
    description="API để phân tích mã nhằm phát hiện mẫu do AI tạo vs mẫu viết bởi con người",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], #FIXME: Tạm thời cho phép tất cả origin
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

if ANALYSIS_MODULES_AVAILABLE:
    advanced_extractor = AdvancedFeatureExtractor()
    ast_analyzer = CppASTAnalyzer()
    human_style_analyzer = HumanStyleAnalyzer()
    try:
        detection_model = create_detector("enhanced")
    except Exception as e:
        detection_model = create_detector("heuristic")
else:
    advanced_extractor = AdvancedFeatureExtractor()
    ast_analyzer = CppASTAnalyzer()
    human_style_analyzer = HumanStyleAnalyzer()
    detection_model = None

class AIAnalyzer:
    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY")
        
        if not self.api_key:
            print("Chưa có GEMINI_API_KEY")
            self.client = None
        elif not GENAI_AVAILABLE:
            self.client = None
        else:
            try:
                self.client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
            except Exception as e:
                self.client = None
    
    async def analyze_code(self, code: str, filename: str = "", language: str = "c") -> Dict:
        if not self.client:
            return {
                "success": False,
                "error": "GenAI client không khả dụng (thiếu API key hoặc library)",
                "mdx_content": "",
                "model": "unknown"
            }
        
        try:
            # NOTE: Prompt để phân tích
            prompt = f"""
You are an AI Code Detector designed to analyze and detect if a given piece of code has been generated by ChatGPT or another AI model. Please analyze the following {language.upper()} code and provide comprehensive insights based on coding style, structure, and syntax that are indicative of AI-generated versus human-written code.

**Instructions:**
- Analyze coding patterns, structural elements, and stylistic choices
- Identify specific elements typical of AI models vs human-written code  
- Provide probabilities rather than absolute determinations
- Explain reasoning with specific evidence from the code
- Be professional and detailed in your analysis
- Respond in Vietnamese language with clear, structured explanations

**Code to analyze:**
```{language}
{code}
```

**Analysis Criteria:**
- **Phong cách mã nguồn**: Consistency, naming conventions, formatting patterns
- **Cấu trúc code**: Logic flow, organization, complexity patterns  
- **Patterns cú pháp**: AI-typical vs human-typical syntax usage
- **Comments và documentation**: Quality, style, and documentation patterns
- **Xử lý lỗi**: Error handling approaches and patterns
- **Best practices**: Adherence to coding standards and conventions
- **AI indicators**: Specific patterns commonly found in AI-generated code
- **Human indicators**: Natural inconsistencies and personal coding habits

**Return your analysis in MDX format with the following structure:**

# Phân tích Code AI Detection

## Kết quả dự đoán
**Dự đoán:** [Code do AI viết hoặc Code do người viết]  
**Độ tin cậy:** [0-100]%  
**Xác suất là AI viết:** [0-100]%  
**Xác suất là người viết:** [0-100]%  

## Phân tích chi tiết

### Đánh giá phong cách mã nguồn
[Phân tích chi tiết về coding style, naming conventions, formatting]

### Đánh giá cấu trúc code
[Phân tích về logic flow, organization, complexity patterns]

### Đánh giá patterns cú pháp
[Phân tích về AI-typical vs human-typical syntax usage]

### Đánh giá documentation
[Phân tích về comments, documentation style]

## Chỉ số quan trọng

### Patterns AI được phát hiện
- [Pattern 1 nếu có]
- [Pattern 2 nếu có]

### Patterns Human được phát hiện  
- [Pattern 1 nếu có]
- [Pattern 2 nếu có]

## Lý do chính

1. **[Lý do 1]**: [Giải thích chi tiết]
2. **[Lý do 2]**: [Giải thích chi tiết]  
3. **[Lý do 3]**: [Giải thích chi tiết]

## Giải thích độ tin cậy
[Giải thích tại sao có độ tin cậy này, dựa trên những evidence nào]

## Ghi chú bổ sung
[Ghi chú thêm nếu cần thông tin bổ sung để phân tích chính xác hơn]

---
*Phân tích được thực hiện bởi AI Code Detector với focus vào patterns đặc trưng của AI vs Human code*
"""

            def generate_content():
                response = self.client.models.generate_content(
                    model="gemini-2.0-flash",
                    contents=prompt
                )
                return response.text
            
            content = await asyncio.to_thread(generate_content)
            
            if content and content.strip():
                return {
                    "success": True,
                    "mdx_content": content,
                    "analysis_type": "mdx",
                    "model": "gemini-2.0-flash",
                    "raw_response": content
                }
            else:
                return {
                    "success": False,
                    "error": "No content generated from GenAI",
                    "mdx_content": "",
                    "model": "gemini-2.0-flash"
                }
                        
        except Exception as e:
            return {
                "success": False,
                "error": f"AI analysis failed: {str(e)}",
                "mdx_content": "",
                "model": "gemini-2.0-flash",
                "details": str(e)
            }
    
ai_analyzer = AIAnalyzer()

class CodeAnalysisRequest(BaseModel):
    code: str = Field(..., min_length=1, max_length=50000, description="Mã nguồn")
    filename: Optional[str] = Field("code.c", description="Tên file")
    language: str = Field("c", description="Ngôn ngữ")
    
    @validator('code')
    def validate_code(cls, v):
        if not v.strip():
            raise ValueError("Mã không thể để trống")
        return v
    
    @validator('language')
    def validate_language(cls, v):
        supported_languages = ["c", "cpp", "c++"]
        if v.lower() not in supported_languages:
            raise ValueError(f"Ngôn ngữ phải là một trong: {supported_languages}")
        return v.lower()

class BaselineComparison(BaseModel): 
    ai_baseline: float
    human_baseline: float
    current_value: float
    deviation_from_ai: float  
    deviation_from_human: float  
    ai_similarity: float  
    human_similarity: float  
    verdict: str  
    confidence: float  
    explanation: str

class FeatureInfo(BaseModel):
    name: str
    value: float
    normalized: bool
    interpretation: str
    weight: float = 1.0
    baseline_comparison: Optional[BaselineComparison] = None

class FeatureGroup(BaseModel):
    group_name: str
    description: str
    features: List[FeatureInfo]
    group_score: float
    visualization_type: str  

class BaselineSummary(BaseModel):
    total_features_compared: int
    ai_like_features: int
    human_like_features: int
    neutral_features: int
    strongest_ai_indicators: List[str]  
    strongest_human_indicators: List[str]  
    overall_ai_similarity: float
    overall_human_similarity: float

class AssessmentResult(BaseModel):
    overall_score: float = Field(..., ge=0, le=1, description="0=giống human, 1=giống AI")
    confidence: float = Field(..., ge=0, le=1, description="Mức độ tin cậy")
    key_indicators: List[str]
    summary: str
    baseline_summary: Optional[BaselineSummary] = None

class CodeInfo(BaseModel):
    filename: str
    language: str
    loc: int
    file_size: int

class AnalysisResponse(BaseModel):
    success: bool
    analysis_id: str
    timestamp: str
    code_info: CodeInfo
    feature_groups: Dict[str, FeatureGroup]
    assessment: AssessmentResult
    raw_features: Optional[Dict[str, float]] = None

class FileAnalysisResult(BaseModel):
    filename: str
    filepath: str
    language: str
    loc: int
    file_size: int
    ai_similarity: float
    human_similarity: float
    confidence: float
    analysis_id: str
    status: str  # "success", "error", "processing"
    code_content: Optional[str] = None # NOTE: Trả code đọc được từ Google Drive -> FE
    error_message: Optional[str] = None

class BatchAnalysisRequest(BaseModel):
    source_type: str = Field(..., description="Type of source: 'zip' or 'google_drive'")
    google_drive_url: Optional[str] = Field(None, description="Google Drive share URL")
    batch_name: Optional[str] = Field("Batch Analysis", description="Name for this batch")

    @validator('source_type')
    def validate_source_type(cls, v):
        if v not in ['zip', 'google_drive']:
            raise ValueError("source_type phải là 'zip' hoặc 'google_drive'")
        return v

    @validator('google_drive_url')
    def validate_google_drive_url(cls, v, values):
        if values.get('source_type') == 'google_drive' and not v:
            raise ValueError("google_drive_url là bắt buộc khi source_type là 'google_drive'")
        if v and not re.match(r'https://drive\.google\.com/.*', v):
            raise ValueError("URL không hợp lệ cho Google Drive")
        return v

class BatchAnalysisResponse(BaseModel):
    batch_id: str
    batch_name: str
    total_files: int
    processed_files: int
    success_count: int
    error_count: int
    results: List[FileAnalysisResult]
    status: str  # "processing", "completed", "error"
    created_at: str
    completed_at: Optional[str] = None
    error_message: Optional[str] = None


def generate_analysis_id() -> str:
    return f"analysis_{uuid.uuid4().hex[:12]}"

def generate_batch_id() -> str:
    return f"batch_{uuid.uuid4().hex[:12]}"

def extract_google_drive_id(url: str) -> Optional[str]:
    patterns = [
        r'/file/d/([a-zA-Z0-9_-]+)',  # File URL
        r'/folders/([a-zA-Z0-9_-]+)', # Folder URL
        r'id=([a-zA-Z0-9_-]+)',       # Query parameter
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None

def create_google_drive_service():
    if not GOOGLE_DRIVE_AVAILABLE:
        raise HTTPException(
            status_code=500,
            detail="Google API client not available. Install with: pip install google-api-python-client"
        )

    api_key = os.getenv("GOOGLE_DRIVE_API_KEY")
    if not api_key:
        raise HTTPException(
            status_code=500,
            detail="GOOGLE_DRIVE_API_KEY environment variable not set"
        )

    try:
        # Create Drive API service with API key
        service = googleapiclient.discovery.build(
            'drive',
            'v3',
            developerKey=api_key
        )
        return service

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to create Google Drive service: {str(e)}"
        )

def get_google_drive_files_info(service, folder_id: str, base_path: str = "") -> List[Dict[str, str]]:
    files_info = []

    try:
        query = f"'{folder_id}' in parents and trashed = false"
        results = service.files().list(
            q=query,
            fields="files(id, name, mimeType, size)",
            pageSize=1000
        ).execute()

        items = results.get('files', [])

        for item in items:
            file_name = item['name']
            file_id = item['id']
            mime_type = item['mimeType']
            file_size = int(item.get('size', 0))

            if mime_type == 'application/vnd.google-apps.folder':
                subfolder_files = get_google_drive_files_info(
                    service,
                    file_id,
                    f"{base_path}{file_name}/"
                )
                files_info.extend(subfolder_files)
            else:
                if any(file_name.endswith(ext) for ext in ['.c', '.cpp', '.cc', '.cxx', '.h', '.hpp', '.txt']):
                    files_info.append({
                        'filename': file_name,
                        'file_id': file_id,
                        'filepath': f"{base_path}{file_name}",
                        'language': get_file_language(file_name),
                        'size': file_size
                    })

    except Exception as e:
        print(f"Error getting files from folder {folder_id}: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to access Google Drive folder: {str(e)}"
        )

    return files_info

async def download_google_drive_files(files_info: List[Dict[str, str]], extract_to: str) -> List[Dict[str, str]]:
    if not GOOGLE_DRIVE_AVAILABLE:
        raise HTTPException(
            status_code=500,
            detail="Google API client not available"
        )

    service = create_google_drive_service()
    downloaded_files = []

    async def download_single_file(file_info: Dict[str, str]):
        try:
            local_path = os.path.join(extract_to, file_info['filename'])

            request = service.files().get_media(fileId=file_info['file_id'])
            with open(local_path, 'wb') as fh:
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while done is False:
                    status, done = downloader.next_chunk()
                    print(f"Download {file_info['filename']}: {int(status.progress() * 100)}%.")

            return {
                'filename': file_info['filename'],
                'filepath': file_info['filepath'],
                'extracted_path': local_path,
                'language': file_info['language']
            }

        except Exception as e:
            print(f"Failed to download {file_info['filename']}: {e}")
            if os.path.exists(local_path):
                os.remove(local_path)
            return None

    semaphore = asyncio.Semaphore(3)

    async def limited_download(file_info):
        async with semaphore:
            return await download_single_file(file_info)

    tasks = [limited_download(file_info) for file_info in files_info]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    for result in results:
        if isinstance(result, dict) and result is not None:
            downloaded_files.append(result)

    return downloaded_files

def get_file_language(filename: str) -> str:
    ext = Path(filename).suffix.lower()
    language_map = {
        '.c': 'c',
        '.cpp': 'cpp',
        '.cc': 'cpp',
        '.cxx': 'cpp',
        '.h': 'c',
        '.hpp': 'cpp',
        '.txt': 'c'
    }
    return language_map.get(ext, 'c')

def extract_files_from_archive(archive_path: str, extract_to: str) -> List[Dict[str, str]]:
    extracted_files = []

    try:
        if archive_path.endswith('.zip'):
            with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                for file_info in zip_ref.filelist:
                    if not file_info.is_dir():
                        filename = file_info.filename
                        if any(filename.endswith(ext) for ext in ['.c', '.cpp', '.cc', '.cxx', '.h', '.hpp', '.txt']):
                            extracted_path = zip_ref.extract(file_info, extract_to)
                            extracted_files.append({
                                'filename': Path(filename).name,
                                'filepath': filename,
                                'extracted_path': extracted_path,
                                'language': get_file_language(filename)
                            })

        elif archive_path.endswith('.rar'):
            if not RARFILE_AVAILABLE:
                raise HTTPException(
                    status_code=400,
                    detail="RAR file support not available. Please install rarfile module."
                )
            with rarfile.RarFile(archive_path, 'r') as rar_ref:
                for file_info in rar_ref.infolist():
                    if not file_info.isdir():
                        filename = file_info.filename
                        if any(filename.endswith(ext) for ext in ['.c', '.cpp', '.cc', '.cxx', '.h', '.hpp', '.txt']):
                            extracted_path = rar_ref.extract(file_info, extract_to)
                            extracted_files.append({
                                'filename': Path(filename).name,
                                'filepath': filename,
                                'extracted_path': str(extracted_path) if isinstance(extracted_path, Path) else extracted_path,
                                'language': get_file_language(filename)
                            })

    except Exception as e:
        print(f"Lỗi extract archive: {e}")
        raise HTTPException(
            status_code=400,
            detail=f"Không thể extract file: {str(e)}"
        )

    return extracted_files

async def analyze_file_batch(files_info: List[Dict[str, str]]) -> List[FileAnalysisResult]:
    results = []

    async def analyze_single_file(file_info: Dict[str, str]) -> FileAnalysisResult:
        try:
            async with aiofiles.open(file_info['extracted_path'], 'r', encoding='utf-8') as f:
                content = await f.read()

            if not content.strip():
                return FileAnalysisResult(
                    filename=file_info['filename'],
                    filepath=file_info['filepath'],
                    language=file_info['language'],
                    loc=0,
                    file_size=0,
                    ai_similarity=0.0,
                    human_similarity=0.0,
                    confidence=0.0,
                    analysis_id="",
                    status="error",
                    code_content=None,
                    error_message="File trống"
                )

            # Create analysis request
            analysis_request = CodeAnalysisRequest(
                code=content,
                filename=file_info['filename'],
                language=file_info['language']
            )

            # Perform analysis
            analysis_response = await analyze_code_combined(analysis_request)

            if analysis_response.success:
                assessment = analysis_response.assessment
                ai_similarity = assessment.overall_score * 100
                human_similarity = (1 - assessment.overall_score) * 100

                return FileAnalysisResult(
                    filename=file_info['filename'],
                    filepath=file_info['filepath'],
                    language=file_info['language'],
                    loc=analysis_response.code_info.loc,
                    file_size=analysis_response.code_info.file_size,
                    ai_similarity=round(ai_similarity, 1),
                    human_similarity=round(human_similarity, 1),
                    confidence=round(assessment.confidence, 3),
                    analysis_id=analysis_response.analysis_id,
                    status="success",
                    code_content=content
                )
            else:
                return FileAnalysisResult(
                    filename=file_info['filename'],
                    filepath=file_info['filepath'],
                    language=file_info['language'],
                    loc=len(content.splitlines()),
                    file_size=len(content.encode('utf-8')),
                    ai_similarity=0.0,
                    human_similarity=0.0,
                    confidence=0.0,
                    analysis_id="",
                    status="error",
                    code_content=content if len(content) < 10000 else None,
                    error_message="Phân tích thất bại"
                )

        except Exception as e:
            return FileAnalysisResult(
                filename=file_info['filename'],
                filepath=file_info['filepath'],
                language=file_info['language'],
                loc=0,
                file_size=0,
                ai_similarity=0.0,
                human_similarity=0.0,
                confidence=0.0,
                analysis_id="",
                status="error",
                code_content=None,
                error_message=str(e)
            )
    semaphore = asyncio.Semaphore(5)

    async def limited_analyze(file_info):
        async with semaphore:
            return await analyze_single_file(file_info)

    tasks = [limited_analyze(file_info) for file_info in files_info]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    final_results = []
    for result in results:
        if isinstance(result, Exception):
            final_results.append(FileAnalysisResult(
                filename="unknown",
                filepath="unknown",
                language="c",
                loc=0,
                file_size=0,
                ai_similarity=0.0,
                human_similarity=0.0,
                confidence=0.0,
                analysis_id="",
                status="error",
                code_content=None,
                error_message=str(result)
            ))
        else:
            final_results.append(result)

    return final_results

def calculate_file_size(code: str) -> int:
    return len(code.encode('utf-8'))

def calculate_baseline_comparison(feature_name: str, current_value: float) -> Optional[BaselineComparison]:
    try:
        from baseline_loader import get_baseline_loader
        baseline_loader = get_baseline_loader()
        baseline_stats = baseline_loader.get_baseline_stats()
        baseline = baseline_stats.get_feature_baseline(feature_name)
        if not baseline:
            available_features = list(baseline_stats.ai_stats.keys())[:10]
            return None
            
        ai_baseline, human_baseline = baseline
        
        baseline_range = abs(ai_baseline - human_baseline)
        if baseline_range == 0:
            return None
        ai_distance = abs(current_value - ai_baseline)
        human_distance = abs(current_value - human_baseline)
        ai_distance_norm = min(ai_distance / baseline_range, 2.0) if baseline_range > 0 else 0
        human_distance_norm = min(human_distance / baseline_range, 2.0) if baseline_range > 0 else 0
        
        ai_similarity = max(0, 1.0 - ai_distance_norm)
        human_similarity = max(0, 1.0 - human_distance_norm)
        if ai_baseline < human_baseline:
            deviation_from_ai = (current_value - ai_baseline) / baseline_range
            deviation_from_human = (current_value - human_baseline) / baseline_range
        else:
            deviation_from_ai = (ai_baseline - current_value) / baseline_range
            deviation_from_human = (human_baseline - current_value) / baseline_range
        deviation_from_ai = max(-2.0, min(2.0, deviation_from_ai))
        deviation_from_human = max(-2.0, min(2.0, deviation_from_human))
        if ai_similarity > human_similarity + 0.1:
            verdict = "ai-like"
            confidence = min(0.95, ai_similarity)
        elif human_similarity > ai_similarity + 0.1:
            verdict = "human-like"
            confidence = min(0.95, human_similarity)
        else:
            verdict = "neutral"
            confidence = 0.5
        if verdict == "ai-like":
            explanation = f"Gần baseline AI ({ai_baseline:.3f}) hơn baseline Human ({human_baseline:.3f})"
        elif verdict == "human-like":
            explanation = f"Gần baseline Human ({human_baseline:.3f}) hơn baseline AI ({ai_baseline:.3f})"
        else:
            explanation = f"Nằm giữa baseline AI ({ai_baseline:.3f}) và baseline Human ({human_baseline:.3f})"
        
        return BaselineComparison(
            ai_baseline=ai_baseline,
            human_baseline=human_baseline,
            current_value=current_value,
            deviation_from_ai=deviation_from_ai,
            deviation_from_human=deviation_from_human,
            ai_similarity=ai_similarity,
            human_similarity=human_similarity,
            verdict=verdict,
            confidence=confidence,
            explanation=explanation
        )
        
    except Exception as e:
        print(f"⚠️ Lỗi tính toán so sánh baseline cho {feature_name}: {e}")
        return None

def interpret_feature(feature_name: str, value: float, normalized: bool = True) -> str:
    interpretations = {
        "loc": f"{'Small' if value < 50 else 'Medium' if value < 200 else 'Large'} codebase ({value} lines)",
        "nodes_per_loc": f"{'Simple' if value < 2 else 'Complex'} AST structure ({value:.2f} nodes/line)",
        "max_depth": f"{'Shallow' if value < 5 else 'Deep'} nesting ({value} levels)",
        "cyclomatic_complexity": f"{'Thấp' if value < 5 else 'Trung bình' if value < 15 else 'Cao'} độ phức tạp ({value:.1f})",  
        "spacing_issues_ratio": f"{'Tốt' if value < 0.1 else 'Kém'} tính nhất quán khoảng cách ({value:.1%} lỗi)",
        "indentation_issues_ratio": f"{'Nhất quán' if value < 0.1 else 'Không nhất quán'} thụt lề ({value:.1%} lỗi)",
        "naming_inconsistency_ratio": f"{'Nhất quán' if value < 0.2 else 'Không nhất quán'} đặt tên ({value:.1%} lỗi)",
        "template_usage_score": f"{'Thấp' if value < 0.1 else 'Cao'} sử dụng template ({value:.1%})",
        "boilerplate_ratio": f"{value:.1%} mã boilerplate",
        "error_handling_score": f"{'Tối thiểu' if value < 0.05 else 'Mở rộng'} xử lý lỗi ({value:.1%})",
    }
    
    return interpretations.get(feature_name, f"Giá trị: {value:.3f}")

def create_feature_groups(features_dict: Dict[str, float]) -> Dict[str, FeatureGroup]:
    try:
        from baseline_loader import get_baseline_loader
        baseline_loader = get_baseline_loader()
        baseline_stats = baseline_loader.get_baseline_stats()
        
        available_features = set(baseline_stats.ai_stats.keys()) & set(baseline_stats.human_stats.keys())
        
        usable_features = {f for f in available_features if f in features_dict}
                
    except Exception as e:
        usable_features = set()
    
    def categorize_features(usable_features: set) -> Dict[str, List[str]]:
        if usable_features:
            structure_features = []
            style_features = []
            complexity_features = []
            ai_detection_features = []
            naming_features = []
            
            for feature in usable_features:
                feature_lower = feature.lower()
                
                if any(keyword in feature_lower for keyword in [
                    'ast_', 'nodes', 'depth', 'branching', 'control', 'statements', 
                    'loops', 'functions_per_loc', 'loc', 'recursive'
                ]):
                    structure_features.append(feature)
                
                elif any(keyword in feature_lower for keyword in [
                    'spacing', 'indentation', 'formatting', 'brace', 'operator_spacing',
                    'camel_case', 'snake_case', 'consistency', 'style'
                ]):
                    style_features.append(feature)
                
                elif any(keyword in feature_lower for keyword in [
                    'complexity', 'halstead', 'cognitive', 'maintainability', 
                    'cyclomatic', 'comment_ratio', 'code_to_comment'
                ]):
                    complexity_features.append(feature)
                
                elif any(keyword in feature_lower for keyword in [
                    'naming', 'variable', 'descriptive', 'generic', 'abbreviation',
                    'meaningful_names', 'hungarian', 'magic_numbers'
                ]):
                    naming_features.append(feature)
                
                elif any(keyword in feature_lower for keyword in [
                    'ai_pattern', 'template', 'boilerplate', 'error_handling',
                    'defensive', 'over_engineering', 'redundancy', 'duplicate'
                ]):
                    ai_detection_features.append(feature)
                
                elif feature in ['blank_ratio', 'comment_ratio', 'functions', 'token_count']:
                    structure_features.append(feature)
                
                else:
                    structure_features.append(feature)
            
            return {
                'structure_metrics': structure_features,
                'style_metrics': style_features, 
                'complexity_metrics': complexity_features,
                'naming_metrics': naming_features,
                'ai_detection_metrics': ai_detection_features
            }
        
        else:
            return {
                'structure_metrics': [
                    "loc", "nodes_per_loc", "max_depth", "avg_depth", "branching_factor",
                    "if_statements_per_loc", "for_loops_per_loc", "while_loops_per_loc",
                    "functions_per_loc", "cyclomatic_complexity"
                ],
                'style_metrics': [
                    "spacing_issues_ratio", "indentation_issues_ratio", "naming_inconsistency_ratio",
                    "formatting_issues_ratio", "human_style_overall_score", "indentation_consistency",
                    "brace_style_consistency", "operator_spacing_consistency"
                ],
                'complexity_metrics': [
                    "halstead_complexity", "halstead_per_loc", "cognitive_complexity", 
                    "cognitive_per_loc", "maintainability_index", "code_to_comment_ratio",
                    "variable_uniqueness_ratio", "avg_function_length"
                ],
                'ai_detection_metrics': [
                    "template_usage_score", "boilerplate_ratio", "error_handling_score",
                    "defensive_programming_score", "over_engineering_score", "copy_paste_score",
                    "redundancy_duplicate_line_ratio", "ai_pattern_template_usage_score"
                ]
            }
    
    feature_categories = categorize_features(usable_features)
    
    def create_group(group_name: str, description: str, feature_names: List[str], viz_type: str) -> FeatureGroup:
        features = []
        group_values = []
        
        sorted_features = feature_names.copy()
        try:
            if usable_features:
                baseline_loader = get_baseline_loader()
                def get_effect_size(fname):
                    baseline_stats = baseline_loader.get_baseline_stats()
                    baseline = baseline_stats.get_feature_baseline(fname)
                    if baseline:
                        ai_val, human_val = baseline
                        return abs(ai_val - human_val)
                    return 0
                
                sorted_features.sort(key=get_effect_size, reverse=True)
        except:
            pass
        
        for fname in sorted_features:
            if fname in features_dict:
                value = features_dict[fname]
                
                baseline_comparison = calculate_baseline_comparison(fname, value)
                if baseline_comparison:
                    print(f"✓ So sánh baseline cho {fname}: {baseline_comparison.verdict}")
                else:
                    print(f"❌ Không có so sánh baseline cho {fname}")
                
                features.append(FeatureInfo(
                    name=fname,
                    value=value,
                    normalized=True,
                    interpretation=interpret_feature(fname, value),
                    weight=1.0,
                    baseline_comparison=baseline_comparison
                ))
                group_values.append(value)
        
        if group_values:
            weighted_values = []
            for feature in features:
                if feature.baseline_comparison:
                    weight = feature.baseline_comparison.confidence * abs(
                        feature.baseline_comparison.ai_similarity - feature.baseline_comparison.human_similarity
                    )
                    weighted_values.append(feature.value * weight)
                else:
                    weighted_values.append(feature.value)
            
            group_score = sum(weighted_values) / len(weighted_values) if weighted_values else 0.0
        else:
            group_score = 0.0
            
        group_score = max(0.0, min(1.0, group_score))
        
        return FeatureGroup(
            group_name=group_name,
            description=description,
            features=features,
            group_score=group_score,
            visualization_type=viz_type
        )
    
    groups = {}
    
    if 'structure_metrics' in feature_categories and feature_categories['structure_metrics']:
        groups['structure_metrics'] = create_group(
            "Structure Metrics",
            "Cấu trúc mã, AST analysis và control flow patterns",
            feature_categories['structure_metrics'],
            "bar"
        )
    
    if 'style_metrics' in feature_categories and feature_categories['style_metrics']:
        style_features = feature_categories['style_metrics']
        
        if len(style_features) > 12:
            try:
                baseline_loader = get_baseline_loader()
                def get_style_priority(fname):
                    baseline_stats = baseline_loader.get_baseline_stats()
                    baseline = baseline_stats.get_feature_baseline(fname)
                    if baseline:
                        ai_val, human_val = baseline
                        effect_size = abs(ai_val - human_val)
                        if any(keyword in fname.lower() for keyword in ['spacing', 'indentation', 'consistency']):
                            effect_size *= 1.5
                        return effect_size
                    return 0
                
                style_features = sorted(style_features, key=get_style_priority, reverse=True)[:12]
            except:
                style_features = style_features[:12]
        
        groups['style_metrics'] = create_group(
            "Style Metrics", 
            "Phong cách code, formatting và human-like patterns",
            style_features,
            "radar"
        )
    
    if 'complexity_metrics' in feature_categories and feature_categories['complexity_metrics']:
        groups['complexity_metrics'] = create_group(
            "Complexity Metrics",
            "Độ phức tạp code, maintainability và cognitive load",
            feature_categories['complexity_metrics'],
            "line"
        )
    
    if 'naming_metrics' in feature_categories and feature_categories['naming_metrics']:
        groups['naming_metrics'] = create_group(
            "Naming Patterns",
            "Variable naming, function naming và consistency",
            feature_categories['naming_metrics'],
            "boxplot"
        )
    
    # FIXME: Tạm thời cho viết báo cáo
    # if 'ai_detection_metrics' in feature_categories and feature_categories['ai_detection_metrics']:
    #     groups['ai_detection_metrics'] = create_group(
    #         "AI Detection Metrics",
    #         "Patterns thường gặp trong code của AI",
    #         feature_categories['ai_detection_metrics'],
    #         "boxplot"
    #     )
    
    if not groups and features_dict:
        all_features = list(features_dict.keys())[:20]
        groups['all_features'] = create_group(
            "All Features",
            "Tất cả features có sẵn",
            all_features,
            "bar"
        )
    
    return groups

def calculate_baseline_summary(feature_groups: Dict[str, FeatureGroup]) -> Optional[BaselineSummary]:
    try:
        ai_like_features = []
        human_like_features = []
        neutral_features = []
        ai_similarities = []
        human_similarities = []
        for group in feature_groups.values():
            for feature in group.features:
                if feature.baseline_comparison:
                    comparison = feature.baseline_comparison
                    if comparison.verdict == "ai-like":
                        ai_like_features.append((feature.name, comparison.confidence))
                    elif comparison.verdict == "human-like":
                        human_like_features.append((feature.name, comparison.confidence))
                    else:
                        neutral_features.append(feature.name)
                    ai_similarities.append(comparison.ai_similarity)
                    human_similarities.append(comparison.human_similarity)
        overall_ai_similarity = sum(ai_similarities) / len(ai_similarities) if ai_similarities else 0
        overall_human_similarity = sum(human_similarities) / len(human_similarities) if human_similarities else 0
        ai_like_features.sort(key=lambda x: x[1], reverse=True)
        human_like_features.sort(key=lambda x: x[1], reverse=True)
        
        strongest_ai_indicators = [name for name, _ in ai_like_features[:3]]
        strongest_human_indicators = [name for name, _ in human_like_features[:3]]
        
        return BaselineSummary(
            total_features_compared=len(ai_similarities),
            ai_like_features=len(ai_like_features),
            human_like_features=len(human_like_features),
            neutral_features=len(neutral_features),
            strongest_ai_indicators=strongest_ai_indicators,
            strongest_human_indicators=strongest_human_indicators,
            overall_ai_similarity=round(overall_ai_similarity, 3),
            overall_human_similarity=round(overall_human_similarity, 3)
        )
        
    except Exception as e:
        print(f"Lỗi tính toán tổng quan baseline: {e}")
        return None

def calculate_assessment(feature_groups: Dict[str, FeatureGroup], raw_features: Dict[str, float] = None) -> AssessmentResult:

    if ANALYSIS_MODULES_AVAILABLE and detection_model and raw_features:
        try:
            detection_result = detection_model.detect(raw_features)
            if detection_result.prediction == "AI-generated":
                overall_score = detection_result.confidence
            elif detection_result.prediction == "Human-written":
                overall_score = 1.0 - detection_result.confidence
            else:
                overall_score = 0.5
            key_indicators = []
            for reason in detection_result.reasoning[:4]:
                if "→ AI" in reason:
                    key_indicators.append(f"Mẫu AI: {reason.split(':')[0]}")
                elif "→ Human" in reason:
                    key_indicators.append(f"Mẫu Human: {reason.split(':')[0]}")
            if detection_result.confidence > 0.8:
                key_indicators.append("Dự đoán tin cậy cao")
            elif detection_result.confidence < 0.6:
                key_indicators.append("Tin cậy thấp - mẫu hỗn hợp")
            if detection_result.prediction == "AI-generated":
                if detection_result.confidence > 0.8:
                    summary = f"Phát hiện mẫu AI rõ rệt với độ tin cậy {detection_result.confidence:.1%}"
                else:
                    summary = f"Có thể do AI tạo với độ tin cậy {detection_result.confidence:.1%}"
            elif detection_result.prediction == "Human-written":
                if detection_result.confidence > 0.8:
                    summary = f"Phát hiện mẫu human rõ rệt với độ tin cậy {detection_result.confidence:.1%}"
                else:
                    summary = f"Có thể do human viết với độ tin cậy {detection_result.confidence:.1%}"
            else:
                summary = "Đặc điểm hỗn hợp - cần xem xét thủ công"
            baseline_summary = calculate_baseline_summary(feature_groups)
            
            return AssessmentResult(
                overall_score=round(overall_score, 3),
                confidence=round(detection_result.confidence, 3),
                key_indicators=key_indicators if key_indicators else ["Phân tích hoàn tất thành công"],
                summary=summary,
                baseline_summary=baseline_summary
            )
            
        except Exception as e:
                print(f"Lỗi model phát hiện: {e}")
    weights = {
        "structure_metrics": 0.2,
        "style_metrics": 0.4,
        "complexity_metrics": 0.2,
        "ai_detection_metrics": 0.2
    }
    
    weighted_score = 0.0
    total_weight = 0.0
    key_indicators = []
    
    for group_name, group in feature_groups.items():
        if group_name in weights:
            weight = weights[group_name]
            weighted_score += group.group_score * weight
            total_weight += weight
            if group_name == "style_metrics" and group.group_score > 0.3:
                key_indicators.append("Phát hiện nhiều điểm không nhất quán kiểu human")
            elif group_name == "ai_detection_metrics" and group.group_score > 0.4:
                key_indicators.append("Tìm thấy mẫu đặc trưng AI")
            elif group_name == "complexity_metrics" and group.group_score < 0.2:
                key_indicators.append("Cấu trúc đơn giản bất thường")
    overall_score = weighted_score / total_weight if total_weight > 0 else 0.5
    overall_score = max(0.0, min(1.0, overall_score))
    
    confidence = min(0.9, total_weight)
    if overall_score < 0.3:
        summary = "Mã thể hiện đặc điểm human mạnh mẽ với sự không nhất quán tự nhiên"
    elif overall_score < 0.6:
        summary = "Mã thể hiện đặc điểm hỗn hợp, cần xem xét thủ công"
    else:
        summary = "Mã thể hiện mẫu thường liên quan đến việc AI tạo"
    
    if not key_indicators:
        key_indicators = ["Phân tích hoàn tất thành công"]
    
    baseline_summary = calculate_baseline_summary(feature_groups)
    
    return AssessmentResult(
        overall_score=overall_score,
        confidence=confidence,
        key_indicators=key_indicators,
        summary=summary,
        baseline_summary=baseline_summary
    )

@app.get("/")
async def root():
    return {
        "message": "API Phân tích phát hiện mã AI",
        "status": "đang chạy",
        "version": "1.0.0",
        "analysis_modules": ANALYSIS_MODULES_AVAILABLE
    }

@app.get("/health")
async def health_check():
    return {
        "status": "khỏe mạnh",
        "timestamp": datetime.now().isoformat(),
        "modules": {
            "advanced_features": ANALYSIS_MODULES_AVAILABLE,
            "ast_analyzer": ANALYSIS_MODULES_AVAILABLE,
            "human_style_analyzer": ANALYSIS_MODULES_AVAILABLE
        }
    }

@app.post("/api/analysis/combined-analysis", response_model=AnalysisResponse)
async def analyze_code_combined(request: CodeAnalysisRequest):
    try:
        if not ANALYSIS_MODULES_AVAILABLE:
            raise HTTPException(
                status_code=503,
                detail="Module phân tích không khả dụng. Vui lòng kiểm tra cấu hình server."
            )
        
        analysis_id = generate_analysis_id()
        timestamp = datetime.now().isoformat()
        features = advanced_extractor.extract_all_features(request.code, request.filename)
        if hasattr(features, 'to_dict'):
            features_dict = features.to_dict()
        else:
            features_dict = features
        code_info = CodeInfo(
            filename=request.filename,
            language=request.language,
            loc=features_dict.get('loc', len(request.code.splitlines())),
            file_size=calculate_file_size(request.code)
        )
        
        feature_groups = create_feature_groups(features_dict)
        assessment = calculate_assessment(feature_groups, features_dict)
        response = AnalysisResponse(
            success=True,
            analysis_id=analysis_id,
            timestamp=timestamp,
            code_info=code_info,
            feature_groups=feature_groups,
            assessment=assessment,
            raw_features=features_dict
        )
        
        return response
        
    except Exception as e:
        print(f"Lỗi phân tích: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail=f"Phân tích thất bại: {str(e)}"
        )


@app.post("/api/analysis/upload-file")
async def analyze_uploaded_file(
    file: UploadFile = File(...),
    analysis_type: str = Form("combined"),
    language: str = Form("c")
):
    try:
        MAX_FILE_SIZE = 1024 * 1024  # 1MB
        content = await file.read()

        if len(content) > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413,
                detail=f"File quá lớn. Kích thước tối đa là {MAX_FILE_SIZE/1024/1024}MB"
            )

        allowed_extensions = ['.c', '.cpp', '.cc', '.cxx', '.txt']
        file_extension = Path(file.filename).suffix.lower()

        if file_extension not in allowed_extensions:
            raise HTTPException(
                status_code=400,
                detail=f"Định dạng file không được hỗ trợ. Cho phép: {', '.join(allowed_extensions)}"
            )

        try:
            code_content = content.decode('utf-8')
        except UnicodeDecodeError:
            raise HTTPException(
                status_code=400,
                detail="File phải được mã hóa UTF-8 hợp lệ"
            )

        if not code_content.strip():
            raise HTTPException(
                status_code=400,
                detail="File không thể để trống"
            )

        analysis_request = CodeAnalysisRequest(
            code=code_content,
            filename=file.filename,
            language=language
        )

        if analysis_type == "combined":
            return await analyze_code_combined(analysis_request)
        elif analysis_type == "ai":
            return await analyze_code_with_ai(analysis_request)
        else:
            raise HTTPException(
                status_code=400,
                detail="Loại phân tích không hợp lệ. Phải là: combined hoặc ai"
            )

    except HTTPException:
        raise
    except Exception as e:
        print(f"Lỗi tải lên file: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Phân tích file thất bại: {str(e)}"
        )


@app.get("/api/analysis/methods")
async def get_analysis_methods():
    return {
        "methods": [
            {
                "id": "ai",
                "name": "AI Analysis",
                "description": "Advanced AI analysis with code pattern detection and reasoning",
                "features": ["AI Code Analysis", "Pattern Recognition", "Detailed Assessment", "Reasoning"],
                "estimated_time": "3-8 seconds"
            },
            {
                "id": "combined",
                "name": "Combined Analysis",
                "description": "Feature-based analysis using code structure and baseline comparison",
                "features": ["AST Analysis", "Human Style", "Advanced Features", "AI Detection"],
                "estimated_time": "2-5 seconds"
            },

        ],
        "supported_languages": ["c", "cpp", "c++"],
        "supported_extensions": [".c", ".cpp", ".cc", ".cxx", ".txt"],
        "max_file_size": "1MB",
        "max_code_length": 50000
    }

@app.post("/api/analysis/ai-analysis")
async def analyze_code_with_ai(request: CodeAnalysisRequest):
    try:
        analysis_id = generate_analysis_id()
        timestamp = datetime.now().isoformat()
        ai_result = await ai_analyzer.analyze_code(request.code, request.filename, request.language)        
        code_info = CodeInfo(
            filename=request.filename,
            language=request.language,
            loc=len(request.code.splitlines()),
            file_size=calculate_file_size(request.code)
            )
        
        response = {
            "success": ai_result.get('success', False),
            "analysis_id": analysis_id,
            "timestamp": timestamp,
            "analysis_type": "ai_mdx",
            "code_info": code_info.dict() if hasattr(code_info, 'dict') else {
                "filename": code_info.filename,
                "language": code_info.language,
                "loc": code_info.loc,
                "file_size": code_info.file_size
            },
            "mdx_content": ai_result.get('mdx_content', ''),
            "model": ai_result.get('model', 'unknown'),
            "summary": f"AI analysis {'completed successfully' if ai_result.get('success') else 'failed'}"
        }
        
        return response
        
    except Exception as e:
        print(f"Lỗi phân tích AI: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail=f"Phân tích AI thất bại: {str(e)}"
        )

# FIXME: Sử dụng db cho batch analysis results
batch_results = {}

@app.post("/api/analysis/batch/upload-zip", response_model=BatchAnalysisResponse)
async def analyze_batch_upload(
    file: UploadFile = File(...),
    batch_name: str = Form("Batch Analysis")
):
    try:
        if not file.filename.endswith(('.zip', '.rar')):
            raise HTTPException(
                status_code=400,
                detail="Chỉ hỗ trợ file ZIP hoặc RAR"
            )

        MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
        content = await file.read()
        if len(content) > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413,
                detail=f"File quá lớn. Kích thước tối đa là {MAX_FILE_SIZE/1024/1024}MB"
            )

        with tempfile.TemporaryDirectory() as temp_dir:
            archive_path = Path(temp_dir) / file.filename
            with open(archive_path, 'wb') as f:
                f.write(content)

            extracted_files = extract_files_from_archive(str(archive_path), temp_dir)

            if not extracted_files:
                raise HTTPException(
                    status_code=400,
                    detail="Không tìm thấy file code hợp lệ trong archive"
                )

            batch_id = generate_batch_id()
            created_at = datetime.now().isoformat()

            batch_results[batch_id] = BatchAnalysisResponse(
                batch_id=batch_id,
                batch_name=batch_name,
                total_files=len(extracted_files),
                processed_files=0,
                success_count=0,
                error_count=0,
                results=[],
                status="processing",
                created_at=created_at
            )

            asyncio.create_task(process_batch_analysis(batch_id, extracted_files))

            return batch_results[batch_id]

    except HTTPException:
        raise
    except Exception as e:
        print(f"Lỗi upload batch: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Upload thất bại: {str(e)}"
        )

@app.post("/api/analysis/batch/google-drive", response_model=BatchAnalysisResponse)
async def analyze_google_drive(request: BatchAnalysisRequest):
    try:
        if request.source_type != 'google_drive':
            raise HTTPException(
                status_code=400,
                detail="Endpoint này chỉ dành cho Google Drive"
            )

        drive_id = extract_google_drive_id(request.google_drive_url)
        if not drive_id:
            raise HTTPException(
                status_code=400,
                detail="URL Google Drive không hợp lệ"
            )

        service = create_google_drive_service()

        files_info = get_google_drive_files_info(service, drive_id)

        if not files_info:
            raise HTTPException(
                status_code=400,
                detail="Không tìm thấy file code hợp lệ trong Google Drive folder"
            )

        batch_id = generate_batch_id()
        created_at = datetime.now().isoformat()

        batch_results[batch_id] = BatchAnalysisResponse(
            batch_id=batch_id,
            batch_name=request.batch_name,
            total_files=len(files_info),
            processed_files=0,
            success_count=0,
            error_count=0,
            results=[],
            status="processing",
            created_at=created_at
        )


        asyncio.create_task(process_google_drive_analysis(batch_id, files_info))

        return batch_results[batch_id]

    except HTTPException:
        raise
    except Exception as e:
        print(f"Lỗi Google Drive analysis: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Google Drive analysis thất bại: {str(e)}"
        )

async def process_google_drive_analysis(batch_id: str, files_info: List[Dict[str, str]]):
    """Background task to process Google Drive analysis"""
    try:
        print(f"Starting Google Drive analysis {batch_id} with {len(files_info)} files")

        with tempfile.TemporaryDirectory() as temp_dir:
            downloaded_files = await download_google_drive_files(files_info, temp_dir)

            if not downloaded_files:
                batch_results[batch_id].status = "error"
                batch_results[batch_id].error_message = "Không thể download files từ Google Drive"
                batch_results[batch_id].completed_at = datetime.now().isoformat()
                return

            results = await analyze_file_batch(downloaded_files)

            success_count = len([r for r in results if r.status == "success"])
            error_count = len([r for r in results if r.status == "error"])

            batch_results[batch_id].results = results
            batch_results[batch_id].processed_files = len(results)
            batch_results[batch_id].success_count = success_count
            batch_results[batch_id].error_count = error_count
            batch_results[batch_id].status = "completed"
            batch_results[batch_id].completed_at = datetime.now().isoformat()

            print(f"Completed Google Drive analysis {batch_id}: {success_count} success, {error_count} errors")

    except Exception as e:
        print(f"Error in Google Drive analysis {batch_id}: {str(e)}")
        batch_results[batch_id].status = "error"
        batch_results[batch_id].error_message = str(e)
        batch_results[batch_id].completed_at = datetime.now().isoformat()

@app.get("/api/analysis/batch/{batch_id}/status", response_model=BatchAnalysisResponse)
async def get_batch_status(batch_id: str):
    if batch_id not in batch_results:
        raise HTTPException(
            status_code=404,
            detail="Batch ID không tồn tại"
        )

    return batch_results[batch_id]

@app.get("/api/analysis/batch/{batch_id}/results", response_model=BatchAnalysisResponse)
async def get_batch_results(batch_id: str):
    return await get_batch_status(batch_id)

async def process_batch_analysis(batch_id: str, files_info: List[Dict[str, str]]):
    try:
        print(f"Starting batch analysis {batch_id} with {len(files_info)} files")

        results = await analyze_file_batch(files_info)

        success_count = len([r for r in results if r.status == "success"])
        error_count = len([r for r in results if r.status == "error"])

        batch_results[batch_id].results = results
        batch_results[batch_id].processed_files = len(results)
        batch_results[batch_id].success_count = success_count
        batch_results[batch_id].error_count = error_count
        batch_results[batch_id].status = "completed"
        batch_results[batch_id].completed_at = datetime.now().isoformat()

        print(f"Completed batch analysis {batch_id}: {success_count} success, {error_count} errors")

    except Exception as e:
        print(f"Error in batch analysis {batch_id}: {str(e)}")
        batch_results[batch_id].status = "error"
        batch_results[batch_id].error_message = str(e)
        batch_results[batch_id].completed_at = datetime.now().isoformat()

@app.get("/api/analysis/batch/methods")
async def get_batch_methods():
    return {
        "methods": [
            {
                "id": "zip_upload",
                "name": "Upload ZIP/RAR",
                "description": "Upload file nén chứa nhiều file code để phân tích batch",
                "supported_formats": [".zip", ".rar"],
                "supported_languages": ["c", "cpp"],
                "supported_extensions": [".c", ".cpp", ".cc", ".cxx", ".h", ".hpp", ".txt"],
                "max_file_size": "50MB",
                "max_files": 100
            },
            {
                "id": "google_drive",
                "name": "Google Drive Link",
                "description": "Phân tích files từ Google Drive share link (folders và files)",
                "supported_formats": ["Google Drive share links"],
                "supported_languages": ["c", "cpp"],
                "supported_extensions": [".c", ".cpp", ".cc", ".cxx", ".h", ".hpp", ".txt"],
                "features": ["Recursive folder scanning", "Concurrent downloads", "Real-time progress"],
                "note": "Hoàn chỉnh và sẵn sàng sử dụng"
            }
        ]
    }

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )