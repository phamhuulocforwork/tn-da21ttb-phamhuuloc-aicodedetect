from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Union
import time
import tempfile
import os
import sys
from pathlib import Path

# Import ML integration modules
try:
    from .enhanced_ml_integration import analyze_code_with_enhanced_ml, get_enhanced_analyzer
    from .ml_integration import analyze_code_features, detect_ai_code, code_analyzer
    HAS_ENHANCED_ML = True
except ImportError:
    # Fallback if running as script or enhanced ML not available
    try:
        from enhanced_ml_integration import analyze_code_with_enhanced_ml, get_enhanced_analyzer
        from ml_integration import analyze_code_features, detect_ai_code, code_analyzer
        HAS_ENHANCED_ML = True
    except ImportError:
        from ml_integration import analyze_code_features, detect_ai_code, code_analyzer
        HAS_ENHANCED_ML = False
        print("Enhanced ML not available - using basic analysis")

app = FastAPI(
    title="AI Code Detection API",
    description="API for detecting AI-generated code vs Human-written code with enhanced ML features",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # FIXME: C·∫•u h√¨nh cho production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Data Models
class CodeAnalysisRequest(BaseModel):
    code: str = Field(..., description="Source code ƒë·ªÉ ph√¢n t√≠ch")
    language: str = Field(default="cpp", description="Ng√¥n ng·ªØ l·∫≠p tr√¨nh (c, cpp)")
    filename: Optional[str] = Field(default=None, description="T√™n file (t√πy ch·ªçn)")
    detector_type: str = Field(default="hybrid", description="Lo·∫°i detector: rule, ml, hybrid")
    enhanced_analysis: bool = Field(default=True, description="S·ª≠ d·ª•ng enhanced analysis")

class BasicCodeFeatures(BaseModel):
    loc: float = Field(description="Lines of Code")
    token_count: Optional[float] = Field(description="S·ªë l∆∞·ª£ng token")
    cyclomatic_complexity: Optional[float] = Field(description="ƒê·ªô ph·ª©c t·∫°p cyclomatic trung b√¨nh")
    functions: Optional[int] = Field(description="S·ªë l∆∞·ª£ng h√†m")
    comment_ratio: Optional[float] = Field(description="T·ª∑ l·ªá comment")
    blank_ratio: Optional[float] = Field(description="T·ª∑ l·ªá d√≤ng tr·ªëng")

class EnhancedFeatures(BaseModel):
    ast_features: Optional[Dict] = Field(description="AST analysis features")
    redundancy_features: Optional[Dict] = Field(description="Code redundancy features")
    naming_patterns: Optional[Dict] = Field(description="Naming pattern features")
    complexity_features: Optional[Dict] = Field(description="Advanced complexity features")
    ai_patterns: Optional[Dict] = Field(description="AI-specific pattern features")

class DetectionResult(BaseModel):
    prediction: str = Field(description="AI-generated, Human-written, or Uncertain")
    confidence: float = Field(description="ƒê·ªô tin c·∫≠y (0-1)")
    reasoning: List[str] = Field(description="L√Ω do ph√°n ƒëo√°n")
    method_used: str = Field(description="Ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng")

class PerformanceMetrics(BaseModel):
    feature_extraction_time: float = Field(description="Th·ªùi gian tr√≠ch xu·∫•t features (s)")
    detection_time: float = Field(description="Th·ªùi gian ph√°t hi·ªán (s)")
    total_time: float = Field(description="T·ªïng th·ªùi gian x·ª≠ l√Ω (s)")

class AnalysisMetadata(BaseModel):
    enhanced_analysis: bool = Field(description="C√≥ s·ª≠ d·ª•ng enhanced analysis kh√¥ng")
    detector_type: str = Field(description="Lo·∫°i detector ƒë√£ s·ª≠ d·ª•ng")
    available_detectors: List[str] = Field(description="C√°c detector c√≥ s·∫µn")
    feature_count: int = Field(description="S·ªë l∆∞·ª£ng features ƒë√£ tr√≠ch xu·∫•t")
    fallback_reason: Optional[str] = Field(description="L√Ω do fallback (n·∫øu c√≥)")

class EnhancedAnalysisResponse(BaseModel):
    basic_features: BasicCodeFeatures
    enhanced_features: Optional[EnhancedFeatures]
    detection: DetectionResult
    performance: PerformanceMetrics
    meta: AnalysisMetadata

class FeedbackRequest(BaseModel):
    code: str = Field(..., description="Source code ƒë√£ ƒë∆∞·ª£c ph√¢n t√≠ch")
    predicted_label: str = Field(..., description="Nh√£n d·ª± ƒëo√°n t·ª´ h·ªá th·ªëng")
    actual_label: str = Field(..., description="Nh√£n th·ª±c t·∫ø t·ª´ gi·∫£ng vi√™n")
    feedback_notes: Optional[str] = Field(default="", description="Ghi ch√∫ t·ª´ gi·∫£ng vi√™n")

class HealthResponse(BaseModel):
    status: str
    message: str
    timestamp: float
    ml_features_available: bool
    enhanced_ml_available: bool
    uptime: float

class DetectorInfoResponse(BaseModel):
    enhanced_ml_available: bool
    available_detectors: List[str]
    detector_details: Dict[str, Dict]

# Global variables for tracking
start_time = time.time()

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    Ki·ªÉm tra tr·∫°ng th√°i ho·∫°t ƒë·ªông c·ªßa backend API
    """
    ml_available = hasattr(code_analyzer, 'has_advanced_features') and code_analyzer.has_advanced_features if 'code_analyzer' in globals() else False
    
    return HealthResponse(
        status="OK",
        message="AI Code Detection API is running",
        timestamp=time.time(),
        ml_features_available=ml_available,
        enhanced_ml_available=HAS_ENHANCED_ML,
        uptime=time.time() - start_time
    )

@app.get("/")
async def root():
    """
    Root endpoint - chuy·ªÉn h∆∞·ªõng ƒë·∫øn health check
    """
    return {
        "message": "AI Code Detection API v2.0",
        "status": "OK", 
        "enhanced_ml": HAS_ENHANCED_ML,
        "docs": "/docs",
        "health": "/health",
        "timestamp": time.time()
    }

@app.get("/detectors", response_model=DetectorInfoResponse)
async def get_detector_info():
    """
    L·∫•y th√¥ng tin v·ªÅ c√°c detector c√≥ s·∫µn
    """
    if HAS_ENHANCED_ML:
        analyzer = get_enhanced_analyzer()
        return analyzer.get_detector_info()
    else:
        return DetectorInfoResponse(
            enhanced_ml_available=False,
            available_detectors=["basic-rule"],
            detector_details={
                "basic-rule": {
                    "name": "Basic Rule-based Detector",
                    "type": "rule",
                    "available": True
                }
            }
        )

@app.post("/analyze-code", response_model=EnhancedAnalysisResponse)
async def analyze_code(request: CodeAnalysisRequest):
    """
    Ph√¢n t√≠ch code v·ªõi enhanced ML features
    
    Workflow:
    1. Validate input
    2. Extract comprehensive features (n·∫øu enhanced=True)
    3. Run detection v·ªõi specified detector
    4. Return detailed results v·ªõi performance metrics
    """
    try:
        # Validate input
        if not request.code.strip():
            raise HTTPException(status_code=400, detail="Code kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng")
        
        if request.language not in ["c", "cpp"]:
            raise HTTPException(status_code=400, detail="Ch·ªâ h·ªó tr·ª£ ng√¥n ng·ªØ C v√† C++")
        
        # Enhanced analysis
        if request.enhanced_analysis and HAS_ENHANCED_ML:
            result = analyze_code_with_enhanced_ml(
                code=request.code,
                language=request.language,
                filename=request.filename,
                detector_type=request.detector_type
            )
            
            # Convert to response format
            response = EnhancedAnalysisResponse(
                basic_features=BasicCodeFeatures(**result['basic_features']),
                enhanced_features=EnhancedFeatures(**result['enhanced_features']) if result['enhanced_features'] else None,
                detection=DetectionResult(**result['detection']),
                performance=PerformanceMetrics(**result['performance']),
                meta=AnalysisMetadata(**result['meta'])
            )
            
            return response
        
        # Fallback to basic analysis
        else:
            # Use existing basic analysis
            features_dict = analyze_code_features(request.code, request.language, request.filename)
            
            # Convert to basic features
            basic_features = BasicCodeFeatures(
                loc=features_dict.get("loc", 0),
                token_count=features_dict.get("token_count"),
                cyclomatic_complexity=features_dict.get("cyclomatic_avg"),
                functions=features_dict.get("functions"),
                comment_ratio=features_dict.get("comment_ratio", 0),
                blank_ratio=features_dict.get("blank_ratio", 0)
            )
            
            # Run basic detection
            prediction, confidence, reasoning = detect_ai_code(request.code, features_dict)
            
            detection = DetectionResult(
                prediction=prediction,
                confidence=confidence,
                reasoning=reasoning,
                method_used="basic-rule"
            )
            
            performance = PerformanceMetrics(
                feature_extraction_time=0.001,
                detection_time=0.001,
                total_time=0.002
            )
            
            meta = AnalysisMetadata(
                enhanced_analysis=False,
                detector_type="basic-rule",
                available_detectors=["basic-rule"],
                feature_count=len(features_dict),
                fallback_reason="Enhanced ML not available or disabled"
            )
            
            return EnhancedAnalysisResponse(
                basic_features=basic_features,
                enhanced_features=None,
                detection=detection,
                performance=performance,
                meta=meta
            )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω: {str(e)}")

@app.post("/analyze-code/file")
async def analyze_code_file(file: UploadFile = File(...), 
                           detector_type: str = "hybrid",
                           enhanced_analysis: bool = True):
    """
    Ph√¢n t√≠ch code t·ª´ file upload
    """
    try:
        # Validate file type
        if not file.filename:
            raise HTTPException(status_code=400, detail="Filename kh√¥ng h·ª£p l·ªá")
        
        file_ext = Path(file.filename).suffix.lower()
        if file_ext not in [".c", ".cpp", ".h", ".hpp"]:
            raise HTTPException(status_code=400, detail="Ch·ªâ h·ªó tr·ª£ file C/C++ (.c, .cpp, .h, .hpp)")
        
        # Read file content
        content = await file.read()
        try:
            code = content.decode('utf-8')
        except UnicodeDecodeError:
            raise HTTPException(status_code=400, detail="File kh√¥ng th·ªÉ ƒë·ªçc ƒë∆∞·ª£c (encoding issue)")
        
        # Determine language from extension
        language = "cpp" if file_ext in [".cpp", ".hpp"] else "c"
        
        # Create analysis request
        request = CodeAnalysisRequest(
            code=code,
            language=language,
            filename=file.filename,
            detector_type=detector_type,
            enhanced_analysis=enhanced_analysis
        )
        
        return await analyze_code(request)
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω file: {str(e)}")

@app.post("/analyze-code/batch")
async def analyze_code_batch(requests: List[CodeAnalysisRequest]):
    """
    Ph√¢n t√≠ch batch nhi·ªÅu code samples
    """
    if len(requests) > 10:
        raise HTTPException(status_code=400, detail="Maximum 10 samples per batch")
    
    results = []
    
    for i, request in enumerate(requests):
        try:
            result = await analyze_code(request)
            results.append({
                "index": i,
                "result": result,
                "status": "success"
            })
        except Exception as e:
            results.append({
                "index": i,
                "error": str(e),
                "status": "failed"
            })
    
    return {
        "batch_results": results,
        "total_samples": len(requests),
        "successful": len([r for r in results if r["status"] == "success"]),
        "failed": len([r for r in results if r["status"] == "failed"])
    }

@app.post("/benchmark-detectors")
async def benchmark_detectors(request: CodeAnalysisRequest):
    """
    Benchmark t·∫•t c·∫£ detectors tr√™n code sample
    """
    if not HAS_ENHANCED_ML:
        raise HTTPException(status_code=501, detail="Enhanced ML required for benchmarking")
    
    try:
        analyzer = get_enhanced_analyzer()
        benchmark_results = analyzer.benchmark_detectors(request.code, request.language)
        
        return {
            "benchmark_results": benchmark_results,
            "timestamp": time.time()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Benchmark failed: {str(e)}")

@app.post("/submit-feedback")
async def submit_feedback(feedback: FeedbackRequest):
    """
    Thu th·∫≠p feedback t·ª´ gi·∫£ng vi√™n ƒë·ªÉ c·∫£i thi·ªán model
    """
    try:
        # TODO: L∆∞u feedback v√†o database ho·∫∑c file ƒë·ªÉ training
        feedback_data = {
            "timestamp": time.time(),
            "code_hash": hash(feedback.code),
            "predicted_label": feedback.predicted_label,
            "actual_label": feedback.actual_label,
            "feedback_notes": feedback.feedback_notes,
            "code_length": len(feedback.code)
        }
        
        # T·∫°m th·ªùi log feedback (sau n√†y c√≥ th·ªÉ l∆∞u v√†o DB)
        print(f"üìù Feedback received: {feedback_data}")
        
        return {
            "message": "Feedback ƒë√£ ƒë∆∞·ª£c ghi nh·∫≠n",
            "status": "success",
            "timestamp": time.time()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói l∆∞u feedback: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)