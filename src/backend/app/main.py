import os
import sys
import traceback
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import tempfile
import json
import aiohttp
import asyncio

from fastapi import FastAPI, HTTPException, UploadFile, File, Form, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator
import uvicorn

try:
    from baseline_loader import get_baseline_loader, reload_baseline_stats
    BASELINE_LOADER_AVAILABLE = True
except ImportError as e:
    BASELINE_LOADER_AVAILABLE = False

# Import chart data helpers
try:
    from chart_helpers import prepare_boxplot_data, prepare_enhanced_chart_data
    CHART_HELPERS_AVAILABLE = True
except ImportError as e:
    CHART_HELPERS_AVAILABLE = False
    print(f"⚠️ Chart helpers not available: {e}")
    
    # Fallback functions
    def prepare_boxplot_data(feature_groups):
        return {}
    
    def prepare_enhanced_chart_data(feature_groups):
        return {"boxplot": {}, "comparison": {}, "distribution": {}, "correlation": {}}

if BASELINE_LOADER_AVAILABLE:
    try:
        test_loader = get_baseline_loader()
        test_stats = test_loader.get_feature_stats_summary()
    except Exception as e:
        BASELINE_LOADER_AVAILABLE = False

current_dir = Path(__file__).parent.absolute()
src_dir = current_dir.parent.parent / "src"
sys.path.insert(0, str(src_dir))

try:
    from features.advanced_features import AdvancedFeatureExtractor, ComprehensiveFeatures
    from features.ast_analyzer import CppASTAnalyzer, ASTFeatures
    from features.human_style_analyzer import HumanStyleAnalyzer, HumanStyleFeatures
    from features.detection_models import create_detector
    ANALYSIS_MODULES_AVAILABLE = True
except ImportError as e:
    ANALYSIS_MODULES_AVAILABLE = False
    class AdvancedFeatureExtractor:
        def extract_all_features(self, code: str, filename: str = "") -> Dict:
            return {"error": "Module phân tích không khả dụng"}
    
    class CppASTAnalyzer:
        def analyze_code(self, code: str, filename: str = "") -> Dict:
            return {"error": "AST analyzer không khả dụng"}
    
    class HumanStyleAnalyzer:
        def analyze_code(self, code: str, filename: str = "") -> Dict:
            return {"error": "Human style analyzer không khả dụng"}

app = FastAPI(
    title="API Phân tích phát hiện mã AI",
    description="API để phân tích mã nhằm phát hiện mẫu do AI tạo vs mẫu viết bởi con người",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], #FIXME: Tạm thời cho phép tất cả origin
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

if ANALYSIS_MODULES_AVAILABLE:
    advanced_extractor = AdvancedFeatureExtractor()
    ast_analyzer = CppASTAnalyzer()
    human_style_analyzer = HumanStyleAnalyzer()
    try:
        detection_model = create_detector("enhanced")
    except Exception as e:
        detection_model = create_detector("heuristic")
else:
    advanced_extractor = AdvancedFeatureExtractor()
    ast_analyzer = CppASTAnalyzer()
    human_style_analyzer = HumanStyleAnalyzer()
    detection_model = None

class GeminiAnalyzer:
    def __init__(self):
        self.api_key = os.getenv('GEMINI_API_KEY')
        self.api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
        
        if not self.api_key:
            print("Chưa có API key")
    
    async def analyze_code(self, code: str, filename: str = "", language: str = "c") -> Dict:
        if not self.api_key:
            return {
                "error": "Chưa có API key",
                "ai_analysis": None,
                "confidence": 0.0,
                "reasoning": "Chưa có API key"
            }
        
        try:
            # NOTE: Prompt để phân tích
            prompt = f"""
You are an AI Code Detector designed to analyze and detect if a given piece of code has been generated by ChatGPT or another AI model. Please analyze the following {language.upper()} code and provide insights based on coding style, structure, and syntax that are indicative of AI-generated code.

Code to analyze:
```{language}
{code}
```

Analysis Instructions:
1. Focus on coding style, structure, and syntax patterns
2. Identify specific elements or patterns typical of AI models vs human-written code
3. Provide probabilities or likelihoods rather than absolute determinations
4. Explain why you believe the code is AI-generated or human-written
5. Be polite and professional in your analysis

Please evaluate based on these criteria:
- **Coding Style**: Consistency, naming conventions, formatting patterns
- **Code Structure**: Logic flow, organization, complexity patterns
- **Syntax Patterns**: AI-typical vs human-typical syntax usage
- **Comments**: Quality, style, and patterns in documentation
- **Error Handling**: Approach and patterns in error management
- **Best Practices**: Adherence to coding standards and conventions
- **AI Indicators**: Specific patterns commonly found in AI-generated code
- **Human Indicators**: Natural inconsistencies and personal coding habits

Return your analysis in JSON format with Vietnamese language responses:
{{
    "prediction": "AI-generated" hoặc "Human-written",
    "confidence": number from 0.0 to 1.0,
    "probability_analysis": {{
        "ai_likelihood": number from 0.0 to 1.0,
        "human_likelihood": number from 0.0 to 1.0,
        "uncertainty_level": number from 0.0 to 1.0
    }},
    "reasoning": ["lý do chi tiết 1", "lý do chi tiết 2", "lý do chi tiết 3"],
    "key_indicators": ["chỉ số quan trọng 1", "chỉ số quan trọng 2"],
    "ai_patterns_detected": ["pattern AI được phát hiện 1", "pattern AI được phát hiện 2"],
    "human_patterns_detected": ["pattern human được phát hiện 1", "pattern human được phát hiện 2"],
    "detailed_analysis": {{
        "style_assessment": "đánh giá chi tiết về phong cách coding",
        "structure_assessment": "đánh giá về cấu trúc và tổ chức code",
        "syntax_assessment": "đánh giá về patterns cú pháp",
        "overall_assessment": "đánh giá tổng quan"
    }},
    "confidence_explanation": "giải thích tại sao có độ tin cậy này",
    "additional_notes": "ghi chú bổ sung nếu cần thêm thông tin để phân tích chính xác hơn"
}}
"""

            headers = {
                "Content-Type": "application/json"
            }
            
            data = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "topK": 1,
                    "topP": 1,
                    "maxOutputTokens": 2048,
                }
            }
            
            async with aiohttp.ClientSession() as session:
                url = f"{self.api_url}?key={self.api_key}"
                async with session.post(url, headers=headers, json=data) as response:
                    if response.status == 200:
                        result = await response.json()
                        
                        if 'candidates' in result and len(result['candidates']) > 0:
                            content = result['candidates'][0]['content']['parts'][0]['text']
                            
                            try:
                                json_start = content.find('{')
                                json_end = content.rfind('}') + 1
                                
                                if json_start != -1 and json_end != -1:
                                    json_str = content[json_start:json_end]
                                    analysis_result = json.loads(json_str)
                                    
                                    return {
                                        "success": True,
                                        "ai_analysis": analysis_result,
                                        "raw_response": content,
                                        "model": "gemini-1.5-flash-latest"
                                    }
                                else:
                                    return {
                                        "success": False,
                                        "error": "Could not parse JSON response",
                                        "raw_response": content,
                                        "fallback_analysis": self._parse_text_response(content)
                                    }
                                    
                            except json.JSONDecodeError as e:
                                return {
                                    "success": False,
                                    "error": f"JSON parsing failed: {str(e)}",
                                    "raw_response": content,
                                    "fallback_analysis": self._parse_text_response(content)
                                }
                        else:
                            return {
                                "success": False,
                                "error": "No response from Gemini",
                                "raw_response": str(result)
                            }
                    else:
                        error_text = await response.text()
                        return {
                            "success": False,
                            "error": f"API request failed: {response.status}",
                            "details": error_text
                        }
                        
        except Exception as e:
            return {
                "success": False,
                "error": f"Gemini analysis failed: {str(e)}",
                "details": str(e)
            }
    
    def _parse_text_response(self, content: str) -> Dict:
        prediction = "Human-written"
        confidence = 0.5
        
        content_lower = content.lower()
        
        ai_indicators = ["ai-generated", "generated by ai", "artificial intelligence", "automated", "consistent patterns"]
        human_indicators = ["human-written", "written by human", "natural variations", "inconsistencies", "personal style"]
        
        ai_score = sum(1 for indicator in ai_indicators if indicator in content_lower)
        human_score = sum(1 for indicator in human_indicators if indicator in content_lower)
        
        if ai_score > human_score:
            prediction = "AI-generated"
            confidence = min(0.8, 0.5 + (ai_score * 0.1))
        elif human_score > ai_score:
            prediction = "Human-written"
            confidence = min(0.8, 0.5 + (human_score * 0.1))
            
        return {
            "prediction": prediction,
            "confidence": confidence,
            "probability_analysis": {
                "ai_likelihood": confidence if prediction == "AI-generated" else 1.0 - confidence,
                "human_likelihood": confidence if prediction == "Human-written" else 1.0 - confidence,
                "uncertainty_level": 1.0 - confidence
            },
            "reasoning": ["Phân tích dựa trên text fallback", f"Phát hiện {ai_score} indicators AI, {human_score} indicators human"],
            "key_indicators": ["Text-based heuristic analysis"],
            "ai_patterns_detected": ["Pattern detection from text analysis"] if ai_score > 0 else [],
            "human_patterns_detected": ["Human pattern detection from text"] if human_score > 0 else [],
            "detailed_analysis": {
                "style_assessment": "Đánh giá dựa trên text analysis",
                "structure_assessment": "Không thể phân tích cấu trúc từ text response",
                "syntax_assessment": "Phân tích cú pháp limited trong fallback mode",
                "overall_assessment": f"Dự đoán {prediction} với độ tin cậy {confidence:.1%}"
            },
            "confidence_explanation": f"Độ tin cậy dựa trên {ai_score + human_score} indicators được phát hiện",
            "additional_notes": "Kết quả này được tạo từ fallback parser do JSON parsing thất bại"
        }

gemini_analyzer = GeminiAnalyzer()

class CodeAnalysisRequest(BaseModel):
    code: str = Field(..., min_length=1, max_length=50000, description="Mã nguồn")
    filename: Optional[str] = Field("code.c", description="Tên file")
    language: str = Field("c", description="Ngôn ngữ")
    
    @validator('code')
    def validate_code(cls, v):
        if not v.strip():
            raise ValueError("Mã không thể để trống")
        return v
    
    @validator('language')
    def validate_language(cls, v):
        supported_languages = ["c", "cpp", "c++"]
        if v.lower() not in supported_languages:
            raise ValueError(f"Ngôn ngữ phải là một trong: {supported_languages}")
        return v.lower()

class BaselineComparison(BaseModel): 
    ai_baseline: float
    human_baseline: float
    current_value: float
    deviation_from_ai: float  
    deviation_from_human: float  
    ai_similarity: float  
    human_similarity: float  
    verdict: str  
    confidence: float  
    explanation: str

class FeatureInfo(BaseModel):
    name: str
    value: float
    normalized: bool
    interpretation: str
    weight: float = 1.0
    baseline_comparison: Optional[BaselineComparison] = None

class FeatureGroup(BaseModel):
    group_name: str
    description: str
    features: List[FeatureInfo]
    group_score: float
    visualization_type: str  

class BaselineSummary(BaseModel):
    total_features_compared: int
    ai_like_features: int
    human_like_features: int
    neutral_features: int
    strongest_ai_indicators: List[str]  
    strongest_human_indicators: List[str]  
    overall_ai_similarity: float
    overall_human_similarity: float

class AssessmentResult(BaseModel):
    overall_score: float = Field(..., ge=0, le=1, description="0=giống human, 1=giống AI")
    confidence: float = Field(..., ge=0, le=1, description="Mức độ tin cậy")
    key_indicators: List[str]
    summary: str
    baseline_summary: Optional[BaselineSummary] = None

class CodeInfo(BaseModel):
    filename: str
    language: str
    loc: int
    file_size: int

class AnalysisResponse(BaseModel):
    success: bool
    analysis_id: str
    timestamp: str
    code_info: CodeInfo
    feature_groups: Dict[str, FeatureGroup]
    assessment: AssessmentResult
    raw_features: Optional[Dict[str, float]] = None


def generate_analysis_id() -> str:
    return f"analysis_{uuid.uuid4().hex[:12]}"

def calculate_file_size(code: str) -> int:
    return len(code.encode('utf-8'))

def calculate_baseline_comparison(feature_name: str, current_value: float) -> Optional[BaselineComparison]:
    try:
        from baseline_loader import get_baseline_loader
        baseline_loader = get_baseline_loader()
        baseline_stats = baseline_loader.get_baseline_stats()
        baseline = baseline_stats.get_feature_baseline(feature_name)
        if not baseline:
            available_features = list(baseline_stats.ai_stats.keys())[:10]
            return None
            
        ai_baseline, human_baseline = baseline
        
        baseline_range = abs(ai_baseline - human_baseline)
        if baseline_range == 0:
            return None
        ai_distance = abs(current_value - ai_baseline)
        human_distance = abs(current_value - human_baseline)
        ai_distance_norm = min(ai_distance / baseline_range, 2.0) if baseline_range > 0 else 0
        human_distance_norm = min(human_distance / baseline_range, 2.0) if baseline_range > 0 else 0
        
        ai_similarity = max(0, 1.0 - ai_distance_norm)
        human_similarity = max(0, 1.0 - human_distance_norm)
        if ai_baseline < human_baseline:
            deviation_from_ai = (current_value - ai_baseline) / baseline_range
            deviation_from_human = (current_value - human_baseline) / baseline_range
        else:
            deviation_from_ai = (ai_baseline - current_value) / baseline_range
            deviation_from_human = (human_baseline - current_value) / baseline_range
        deviation_from_ai = max(-2.0, min(2.0, deviation_from_ai))
        deviation_from_human = max(-2.0, min(2.0, deviation_from_human))
        if ai_similarity > human_similarity + 0.1:
            verdict = "ai-like"
            confidence = min(0.95, ai_similarity)
        elif human_similarity > ai_similarity + 0.1:
            verdict = "human-like"
            confidence = min(0.95, human_similarity)
        else:
            verdict = "neutral"
            confidence = 0.5
        if verdict == "ai-like":
            explanation = f"Gần baseline AI ({ai_baseline:.3f}) hơn baseline Human ({human_baseline:.3f})"
        elif verdict == "human-like":
            explanation = f"Gần baseline Human ({human_baseline:.3f}) hơn baseline AI ({ai_baseline:.3f})"
        else:
            explanation = f"Nằm giữa baseline AI ({ai_baseline:.3f}) và baseline Human ({human_baseline:.3f})"
        
        return BaselineComparison(
            ai_baseline=ai_baseline,
            human_baseline=human_baseline,
            current_value=current_value,
            deviation_from_ai=deviation_from_ai,
            deviation_from_human=deviation_from_human,
            ai_similarity=ai_similarity,
            human_similarity=human_similarity,
            verdict=verdict,
            confidence=confidence,
            explanation=explanation
        )
        
    except Exception as e:
        print(f"⚠️ Lỗi tính toán so sánh baseline cho {feature_name}: {e}")
        return None

def interpret_feature(feature_name: str, value: float, normalized: bool = True) -> str:
    interpretations = {
        "loc": f"{'Small' if value < 50 else 'Medium' if value < 200 else 'Large'} codebase ({value} lines)",
        "nodes_per_loc": f"{'Simple' if value < 2 else 'Complex'} AST structure ({value:.2f} nodes/line)",
        "max_depth": f"{'Shallow' if value < 5 else 'Deep'} nesting ({value} levels)",
        "cyclomatic_complexity": f"{'Thấp' if value < 5 else 'Trung bình' if value < 15 else 'Cao'} độ phức tạp ({value:.1f})",  
        "spacing_issues_ratio": f"{'Tốt' if value < 0.1 else 'Kém'} tính nhất quán khoảng cách ({value:.1%} lỗi)",
        "indentation_issues_ratio": f"{'Nhất quán' if value < 0.1 else 'Không nhất quán'} thụt lề ({value:.1%} lỗi)",
        "naming_inconsistency_ratio": f"{'Nhất quán' if value < 0.2 else 'Không nhất quán'} đặt tên ({value:.1%} lỗi)",
        "template_usage_score": f"{'Thấp' if value < 0.1 else 'Cao'} sử dụng template ({value:.1%})",
        "boilerplate_ratio": f"{value:.1%} mã boilerplate",
        "error_handling_score": f"{'Tối thiểu' if value < 0.05 else 'Mở rộng'} xử lý lỗi ({value:.1%})",
    }
    
    return interpretations.get(feature_name, f"Giá trị: {value:.3f}")

def create_feature_groups(features_dict: Dict[str, float]) -> Dict[str, FeatureGroup]:
    try:
        from baseline_loader import get_baseline_loader
        baseline_loader = get_baseline_loader()
        baseline_stats = baseline_loader.get_baseline_stats()
        
        available_features = set(baseline_stats.ai_stats.keys()) & set(baseline_stats.human_stats.keys())
        
        usable_features = {f for f in available_features if f in features_dict}
                
    except Exception as e:
        usable_features = set()
    
    def categorize_features(usable_features: set) -> Dict[str, List[str]]:
        if usable_features:
            structure_features = []
            style_features = []
            complexity_features = []
            ai_detection_features = []
            naming_features = []
            
            for feature in usable_features:
                feature_lower = feature.lower()
                
                if any(keyword in feature_lower for keyword in [
                    'ast_', 'nodes', 'depth', 'branching', 'control', 'statements', 
                    'loops', 'functions_per_loc', 'loc', 'recursive'
                ]):
                    structure_features.append(feature)
                
                elif any(keyword in feature_lower for keyword in [
                    'spacing', 'indentation', 'formatting', 'brace', 'operator_spacing',
                    'camel_case', 'snake_case', 'consistency', 'style'
                ]):
                    style_features.append(feature)
                
                elif any(keyword in feature_lower for keyword in [
                    'complexity', 'halstead', 'cognitive', 'maintainability', 
                    'cyclomatic', 'comment_ratio', 'code_to_comment'
                ]):
                    complexity_features.append(feature)
                
                elif any(keyword in feature_lower for keyword in [
                    'naming', 'variable', 'descriptive', 'generic', 'abbreviation',
                    'meaningful_names', 'hungarian', 'magic_numbers'
                ]):
                    naming_features.append(feature)
                
                elif any(keyword in feature_lower for keyword in [
                    'ai_pattern', 'template', 'boilerplate', 'error_handling',
                    'defensive', 'over_engineering', 'redundancy', 'duplicate'
                ]):
                    ai_detection_features.append(feature)
                
                elif feature in ['blank_ratio', 'comment_ratio', 'functions', 'token_count']:
                    structure_features.append(feature)
                
                else:
                    structure_features.append(feature)
            
            return {
                'structure_metrics': structure_features,
                'style_metrics': style_features, 
                'complexity_metrics': complexity_features,
                'naming_metrics': naming_features,
                'ai_detection_metrics': ai_detection_features
            }
        
        else:
            return {
                'structure_metrics': [
                    "loc", "nodes_per_loc", "max_depth", "avg_depth", "branching_factor",
                    "if_statements_per_loc", "for_loops_per_loc", "while_loops_per_loc",
                    "functions_per_loc", "cyclomatic_complexity"
                ],
                'style_metrics': [
                    "spacing_issues_ratio", "indentation_issues_ratio", "naming_inconsistency_ratio",
                    "formatting_issues_ratio", "human_style_overall_score", "indentation_consistency",
                    "brace_style_consistency", "operator_spacing_consistency"
                ],
                'complexity_metrics': [
                    "halstead_complexity", "halstead_per_loc", "cognitive_complexity", 
                    "cognitive_per_loc", "maintainability_index", "code_to_comment_ratio",
                    "variable_uniqueness_ratio", "avg_function_length"
                ],
                'ai_detection_metrics': [
                    "template_usage_score", "boilerplate_ratio", "error_handling_score",
                    "defensive_programming_score", "over_engineering_score", "copy_paste_score",
                    "redundancy_duplicate_line_ratio", "ai_pattern_template_usage_score"
                ]
            }
    
    feature_categories = categorize_features(usable_features)
    
    def create_group(group_name: str, description: str, feature_names: List[str], viz_type: str) -> FeatureGroup:
        features = []
        group_values = []
        
        sorted_features = feature_names.copy()
        try:
            if usable_features:
                baseline_loader = get_baseline_loader()
                def get_effect_size(fname):
                    baseline_stats = baseline_loader.get_baseline_stats()
                    baseline = baseline_stats.get_feature_baseline(fname)
                    if baseline:
                        ai_val, human_val = baseline
                        return abs(ai_val - human_val)
                    return 0
                
                sorted_features.sort(key=get_effect_size, reverse=True)
        except:
            pass
        
        for fname in sorted_features:
            if fname in features_dict:
                value = features_dict[fname]
                
                baseline_comparison = calculate_baseline_comparison(fname, value)
                if baseline_comparison:
                    print(f"✓ So sánh baseline cho {fname}: {baseline_comparison.verdict}")
                else:
                    print(f"❌ Không có so sánh baseline cho {fname}")
                
                features.append(FeatureInfo(
                    name=fname,
                    value=value,
                    normalized=True,
                    interpretation=interpret_feature(fname, value),
                    weight=1.0,
                    baseline_comparison=baseline_comparison
                ))
                group_values.append(value)
        
        if group_values:
            weighted_values = []
            for feature in features:
                if feature.baseline_comparison:
                    weight = feature.baseline_comparison.confidence * abs(
                        feature.baseline_comparison.ai_similarity - feature.baseline_comparison.human_similarity
                    )
                    weighted_values.append(feature.value * weight)
                else:
                    weighted_values.append(feature.value)
            
            group_score = sum(weighted_values) / len(weighted_values) if weighted_values else 0.0
        else:
            group_score = 0.0
            
        group_score = max(0.0, min(1.0, group_score))
        
        return FeatureGroup(
            group_name=group_name,
            description=description,
            features=features,
            group_score=group_score,
            visualization_type=viz_type
        )
    
    groups = {}
    
    if 'structure_metrics' in feature_categories and feature_categories['structure_metrics']:
        groups['structure_metrics'] = create_group(
            "Structure Metrics",
            "Cấu trúc mã, AST analysis và control flow patterns",
            feature_categories['structure_metrics'],
            "bar"
        )
    
    if 'style_metrics' in feature_categories and feature_categories['style_metrics']:
        style_features = feature_categories['style_metrics']
        
        if len(style_features) > 12:
            try:
                baseline_loader = get_baseline_loader()
                def get_style_priority(fname):
                    baseline_stats = baseline_loader.get_baseline_stats()
                    baseline = baseline_stats.get_feature_baseline(fname)
                    if baseline:
                        ai_val, human_val = baseline
                        effect_size = abs(ai_val - human_val)
                        if any(keyword in fname.lower() for keyword in ['spacing', 'indentation', 'consistency']):
                            effect_size *= 1.5
                        return effect_size
                    return 0
                
                style_features = sorted(style_features, key=get_style_priority, reverse=True)[:12]
            except:
                style_features = style_features[:12]
        
        groups['style_metrics'] = create_group(
            "Style Metrics", 
            "Phong cách code, formatting và human-like patterns",
            style_features,
            "radar"
        )
    
    if 'complexity_metrics' in feature_categories and feature_categories['complexity_metrics']:
        groups['complexity_metrics'] = create_group(
            "Complexity Metrics",
            "Độ phức tạp code, maintainability và cognitive load",
            feature_categories['complexity_metrics'],
            "line"
        )
    
    if 'naming_metrics' in feature_categories and feature_categories['naming_metrics']:
        groups['naming_metrics'] = create_group(
            "Naming Patterns",
            "Variable naming, function naming và consistency",
            feature_categories['naming_metrics'],
            "boxplot"
        )
    
    if 'ai_detection_metrics' in feature_categories and feature_categories['ai_detection_metrics']:
        groups['ai_detection_metrics'] = create_group(
            "AI Detection Metrics",
            "Patterns thường gặp trong code của AI",
            feature_categories['ai_detection_metrics'],
            "boxplot"
        )
    
    if not groups and features_dict:
        all_features = list(features_dict.keys())[:20]
        groups['all_features'] = create_group(
            "All Features",
            "Tất cả features có sẵn",
            all_features,
            "bar"
        )
    
    return groups

def calculate_baseline_summary(feature_groups: Dict[str, FeatureGroup]) -> Optional[BaselineSummary]:
    try:
        ai_like_features = []
        human_like_features = []
        neutral_features = []
        ai_similarities = []
        human_similarities = []
        for group in feature_groups.values():
            for feature in group.features:
                if feature.baseline_comparison:
                    comparison = feature.baseline_comparison
                    if comparison.verdict == "ai-like":
                        ai_like_features.append((feature.name, comparison.confidence))
                    elif comparison.verdict == "human-like":
                        human_like_features.append((feature.name, comparison.confidence))
                    else:
                        neutral_features.append(feature.name)
                    ai_similarities.append(comparison.ai_similarity)
                    human_similarities.append(comparison.human_similarity)
        overall_ai_similarity = sum(ai_similarities) / len(ai_similarities) if ai_similarities else 0
        overall_human_similarity = sum(human_similarities) / len(human_similarities) if human_similarities else 0
        ai_like_features.sort(key=lambda x: x[1], reverse=True)
        human_like_features.sort(key=lambda x: x[1], reverse=True)
        
        strongest_ai_indicators = [name for name, _ in ai_like_features[:3]]
        strongest_human_indicators = [name for name, _ in human_like_features[:3]]
        
        return BaselineSummary(
            total_features_compared=len(ai_similarities),
            ai_like_features=len(ai_like_features),
            human_like_features=len(human_like_features),
            neutral_features=len(neutral_features),
            strongest_ai_indicators=strongest_ai_indicators,
            strongest_human_indicators=strongest_human_indicators,
            overall_ai_similarity=round(overall_ai_similarity, 3),
            overall_human_similarity=round(overall_human_similarity, 3)
        )
        
    except Exception as e:
        print(f"Lỗi tính toán tổng quan baseline: {e}")
        return None

def calculate_assessment(feature_groups: Dict[str, FeatureGroup], raw_features: Dict[str, float] = None) -> AssessmentResult:

    if ANALYSIS_MODULES_AVAILABLE and detection_model and raw_features:
        try:
            detection_result = detection_model.detect(raw_features)
            if detection_result.prediction == "AI-generated":
                overall_score = detection_result.confidence
            elif detection_result.prediction == "Human-written":
                overall_score = 1.0 - detection_result.confidence
            else:
                overall_score = 0.5
            key_indicators = []
            for reason in detection_result.reasoning[:4]:
                if "→ AI" in reason:
                    key_indicators.append(f"Mẫu AI: {reason.split(':')[0]}")
                elif "→ Human" in reason:
                    key_indicators.append(f"Mẫu Human: {reason.split(':')[0]}")
            if detection_result.confidence > 0.8:
                key_indicators.append("Dự đoán tin cậy cao")
            elif detection_result.confidence < 0.6:
                key_indicators.append("Tin cậy thấp - mẫu hỗn hợp")
            if detection_result.prediction == "AI-generated":
                if detection_result.confidence > 0.8:
                    summary = f"Phát hiện mẫu AI rõ rệt với độ tin cậy {detection_result.confidence:.1%}"
                else:
                    summary = f"Có thể do AI tạo với độ tin cậy {detection_result.confidence:.1%}"
            elif detection_result.prediction == "Human-written":
                if detection_result.confidence > 0.8:
                    summary = f"Phát hiện mẫu human rõ rệt với độ tin cậy {detection_result.confidence:.1%}"
                else:
                    summary = f"Có thể do human viết với độ tin cậy {detection_result.confidence:.1%}"
            else:
                summary = "Đặc điểm hỗn hợp - cần xem xét thủ công"
            baseline_summary = calculate_baseline_summary(feature_groups)
            
            return AssessmentResult(
                overall_score=round(overall_score, 3),
                confidence=round(detection_result.confidence, 3),
                key_indicators=key_indicators if key_indicators else ["Phân tích hoàn tất thành công"],
                summary=summary,
                baseline_summary=baseline_summary
            )
            
        except Exception as e:
                print(f"Lỗi model phát hiện: {e}")
    weights = {
        "structure_metrics": 0.2,
        "style_metrics": 0.4,
        "complexity_metrics": 0.2,
        "ai_detection_metrics": 0.2
    }
    
    weighted_score = 0.0
    total_weight = 0.0
    key_indicators = []
    
    for group_name, group in feature_groups.items():
        if group_name in weights:
            weight = weights[group_name]
            weighted_score += group.group_score * weight
            total_weight += weight
            if group_name == "style_metrics" and group.group_score > 0.3:
                key_indicators.append("Phát hiện nhiều điểm không nhất quán kiểu human")
            elif group_name == "ai_detection_metrics" and group.group_score > 0.4:
                key_indicators.append("Tìm thấy mẫu đặc trưng AI")
            elif group_name == "complexity_metrics" and group.group_score < 0.2:
                key_indicators.append("Cấu trúc đơn giản bất thường")
    overall_score = weighted_score / total_weight if total_weight > 0 else 0.5
    overall_score = max(0.0, min(1.0, overall_score))
    
    confidence = min(0.9, total_weight)
    if overall_score < 0.3:
        summary = "Mã thể hiện đặc điểm human mạnh mẽ với sự không nhất quán tự nhiên"
    elif overall_score < 0.6:
        summary = "Mã thể hiện đặc điểm hỗn hợp, cần xem xét thủ công"
    else:
        summary = "Mã thể hiện mẫu thường liên quan đến việc AI tạo"
    
    if not key_indicators:
        key_indicators = ["Phân tích hoàn tất thành công"]
    
    baseline_summary = calculate_baseline_summary(feature_groups)
    
    return AssessmentResult(
        overall_score=overall_score,
        confidence=confidence,
        key_indicators=key_indicators,
        summary=summary,
        baseline_summary=baseline_summary
    )

@app.get("/")
async def root():
    return {
        "message": "API Phân tích phát hiện mã AI",
        "status": "đang chạy",
        "version": "1.0.0",
        "analysis_modules": ANALYSIS_MODULES_AVAILABLE
    }

@app.get("/health")
async def health_check():
    return {
        "status": "khỏe mạnh",
        "timestamp": datetime.now().isoformat(),
        "modules": {
            "advanced_features": ANALYSIS_MODULES_AVAILABLE,
            "ast_analyzer": ANALYSIS_MODULES_AVAILABLE,
            "human_style_analyzer": ANALYSIS_MODULES_AVAILABLE
        }
    }

@app.post("/api/analysis/combined-analysis", response_model=AnalysisResponse)
async def analyze_code_combined(request: CodeAnalysisRequest):
    try:
        if not ANALYSIS_MODULES_AVAILABLE:
            raise HTTPException(
                status_code=503,
                detail="Module phân tích không khả dụng. Vui lòng kiểm tra cấu hình server."
            )
        
        analysis_id = generate_analysis_id()
        timestamp = datetime.now().isoformat()
        features = advanced_extractor.extract_all_features(request.code, request.filename)
        if hasattr(features, 'to_dict'):
            features_dict = features.to_dict()
        else:
            features_dict = features
        code_info = CodeInfo(
            filename=request.filename,
            language=request.language,
            loc=features_dict.get('loc', len(request.code.splitlines())),
            file_size=calculate_file_size(request.code)
        )
        
        feature_groups = create_feature_groups(features_dict)
        assessment = calculate_assessment(feature_groups, features_dict)
        response = AnalysisResponse(
            success=True,
            analysis_id=analysis_id,
            timestamp=timestamp,
            code_info=code_info,
            feature_groups=feature_groups,
            assessment=assessment,
            raw_features=features_dict
        )
        
        return response
        
    except Exception as e:
        print(f"Lỗi phân tích: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail=f"Phân tích thất bại: {str(e)}"
        )

@app.post("/api/analysis/ast-analysis")
async def analyze_ast_only(request: CodeAnalysisRequest):
    try:
        if not ANALYSIS_MODULES_AVAILABLE:
            raise HTTPException(
                status_code=503,
                detail="Module phân tích không khả dụng"
            )
        
        analysis_id = generate_analysis_id()
        timestamp = datetime.now().isoformat()
        ast_features = ast_analyzer.analyze_code(request.code, request.filename)
        if hasattr(ast_features, '__dict__'):
            features_dict = ast_features.__dict__
        else:
            features_dict = ast_features
        response = {
            "success": True,
            "analysis_id": analysis_id,
            "timestamp": timestamp,
            "analysis_type": "ast_only",
            "code_info": {
                "filename": request.filename,
                "language": request.language,
                "loc": features_dict.get('total_nodes', len(request.code.splitlines())),
                "file_size": calculate_file_size(request.code)
            },
            "features": features_dict,
            "summary": f"Phân tích AST hoàn tất với {len(features_dict)} đặc trưng được trích xuất"
        }
        
        return response
        
    except Exception as e:
        print(f"Lỗi phân tích AST: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Phân tích AST thất bại: {str(e)}")

@app.post("/api/analysis/human-style")  
async def analyze_human_style_only(request: CodeAnalysisRequest):
    try:
        if not ANALYSIS_MODULES_AVAILABLE:
            raise HTTPException(
                status_code=503,
                detail="Module phân tích không khả dụng"
            )
        
        analysis_id = generate_analysis_id()
        timestamp = datetime.now().isoformat()
        style_features = human_style_analyzer.analyze_code(request.code, request.filename)
        if hasattr(style_features, 'to_dict'):
            features_dict = style_features.to_dict()
        elif hasattr(style_features, '__dict__'):
            features_dict = style_features.__dict__
        else:
            features_dict = style_features
        
        response = {
            "success": True,
            "analysis_id": analysis_id,
            "timestamp": timestamp,
            "analysis_type": "human_style_only",
            "code_info": {
                "filename": request.filename,
                "language": request.language,
                "loc": len(request.code.splitlines()),
                "file_size": calculate_file_size(request.code)
            },
            "features": features_dict,
            "summary": f"Phân tích phong cách Human hoàn tất với {len(features_dict)} đặc trưng được trích xuất"
        }
        
        return response
        
    except Exception as e:
        print(f"Lỗi phân tích phong cách Human: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Phân tích phong cách Human thất bại: {str(e)}")

@app.post("/api/analysis/advanced-features")
async def analyze_advanced_features_only(request: CodeAnalysisRequest):
    try:
        if not ANALYSIS_MODULES_AVAILABLE:
            raise HTTPException(
                status_code=503,
                detail="Module phân tích không khả dụng"
            )
        
        analysis_id = generate_analysis_id()
        timestamp = datetime.now().isoformat()
        all_features = advanced_extractor.extract_all_features(request.code, request.filename)
        
        if hasattr(all_features, 'to_dict'):
            all_features_dict = all_features.to_dict()
        else:
            all_features_dict = all_features
        advanced_feature_prefixes = [
            'redundancy_', 'complexity_', 'ai_pattern_', 'naming_', 
            'halstead', 'cognitive', 'maintainability', 'copy_paste'
        ]
        
        advanced_features = {}
        for key, value in all_features_dict.items():
            if any(key.startswith(prefix) for prefix in advanced_feature_prefixes):
                advanced_features[key] = value
            elif key in ['loc', 'token_count', 'cyclomatic_complexity', 'functions', 'comment_ratio', 'blank_ratio']:
                advanced_features[key] = value
        
        response = {
            "success": True,
            "analysis_id": analysis_id,
            "timestamp": timestamp,
            "analysis_type": "advanced_features_only",
            "code_info": {
                "filename": request.filename,
                "language": request.language,
                "loc": advanced_features.get('loc', len(request.code.splitlines())),
                "file_size": calculate_file_size(request.code)
            },
            "features": advanced_features,
            "summary": f"Phân tích đặc trưng nâng cao hoàn tất với {len(advanced_features)} đặc trưng được trích xuất"
        }
        
        return response
        
    except Exception as e:
        print(f"Lỗi phân tích đặc trưng nâng cao: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Phân tích tính năng nâng cao thất bại: {str(e)}")

@app.post("/api/analysis/upload-file")
async def analyze_uploaded_file(
    file: UploadFile = File(...),
    analysis_type: str = Form("combined"),
    language: str = Form("c")
):
    try:
        MAX_FILE_SIZE = 1024 * 1024
        content = await file.read()
        
        if len(content) > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413,
                detail=f"File quá lớn. Kích thước tối đa là {MAX_FILE_SIZE/1024/1024}MB"
            )
        
        allowed_extensions = ['.c', '.cpp', '.cc', '.cxx', '.txt']
        file_extension = Path(file.filename).suffix.lower()
        
        if file_extension not in allowed_extensions:
            raise HTTPException(
                status_code=400,
                detail=f"Định dạng file không được hỗ trợ. Cho phép: {', '.join(allowed_extensions)}"
            )
        

        try:
            code_content = content.decode('utf-8')
        except UnicodeDecodeError:
            raise HTTPException(
                status_code=400,
                detail="File phải được mã hóa UTF-8 hợp lệ"
            )
        

        if not code_content.strip():
            raise HTTPException(
                status_code=400,
                detail="File không thể để trống"
            )
        

        analysis_request = CodeAnalysisRequest(
            code=code_content,
            filename=file.filename,
            language=language
        )
        

        if analysis_type == "combined":
            return await analyze_code_combined(analysis_request)
        elif analysis_type == "ast":
            return await analyze_ast_only(analysis_request)
        elif analysis_type == "human-style":
            return await analyze_human_style_only(analysis_request)
        elif analysis_type == "advanced":
            return await analyze_advanced_features_only(analysis_request)
        else:
            raise HTTPException(
                status_code=400,
                detail="Loại phân tích không hợp lệ. Phải là: combined, ast, human-style, hoặc advanced"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        print(f"Lỗi tải lên file: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Phân tích file thất bại: {str(e)}"
        )

@app.get("/api/analysis/methods")
async def get_analysis_methods():
    return {
        "methods": [
            {
                "id": "gemini",
                "name": "Gemini AI Analysis",
                "description": "Advanced AI analysis using Google Gemini with code pattern detection",
                "features": ["AI Code Analysis", "Pattern Recognition", "Detailed Assessment", "Reasoning"],
                "estimated_time": "3-8 seconds"
            },
            {
                "id": "combined",
                "name": "Combined Analysis",
                "description": "Feature-based analysis using code structure and baseline comparison",
                "features": ["AST Analysis", "Human Style", "Advanced Features", "AI Detection"],
                "estimated_time": "2-5 seconds"
            },
            {
                "id": "ast",
                "name": "AST Analysis",
                "description": "Code structure, control flow, and naming pattern analysis",
                "features": ["Structure metrics", "Control flow", "Function analysis", "Variable naming"],
                "estimated_time": "1-2 seconds"
            },
            {
                "id": "human-style", 
                "name": "Human Style Analysis",
                "description": "Coding style and human-like inconsistency detection",
                "features": ["Spacing issues", "Indentation consistency", "Naming patterns", "Formatting"],
                "estimated_time": "1-2 seconds"
            },
            {
                "id": "advanced",
                "name": "Advanced Features",
                "description": "Code complexity, redundancy, and AI pattern detection",
                "features": ["Complexity metrics", "Code redundancy", "AI patterns", "Maintainability"],
                "estimated_time": "2-3 seconds"
            }
        ],
        "supported_languages": ["c", "cpp", "c++"],
        "supported_extensions": [".c", ".cpp", ".cc", ".cxx", ".txt"],
        "max_file_size": "1MB",
        "max_code_length": 50000
    }

@app.get("/api/baseline/stats")
async def get_baseline_stats():
    try:
        from baseline_loader import get_baseline_loader
        baseline_loader = get_baseline_loader()
        stats_summary = baseline_loader.get_feature_stats_summary()
        
        return {
            "success": True,
            "baseline_stats": stats_summary,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Không thể lấy thống kê baseline: {str(e)}"
        )

@app.post("/api/baseline/reload")
async def reload_baseline():
    try:
        from baseline_loader import reload_baseline_stats
        reload_baseline_stats()
        

        global detection_model
        if ANALYSIS_MODULES_AVAILABLE:
            detection_model = create_detector("enhanced")
        
        return {
            "success": True,
            "message": "Thống kê baseline đã được tải lại thành công",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Không thể tải lại thống kê baseline: {str(e)}"
        )

@app.get("/api/baseline/critical-features")
async def get_critical_features():
    try:
        from baseline_loader import get_baseline_loader
        baseline_loader = get_baseline_loader()
        critical_features = baseline_loader.get_critical_features()
        

        formatted_features = []
        for feature_name, config in critical_features.items():
            formatted_features.append({
                "name": feature_name,
                "weight": config["weight"],
                "ai_favored": config["ai_better"],
                "effect_size": config.get("effect_size", 0),
                "description": _get_feature_description(feature_name)
            })
        

        formatted_features.sort(key=lambda x: x["weight"], reverse=True)
        
        return {
            "success": True,
            "critical_features": formatted_features,
            "total_features": len(formatted_features),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Không thể lấy danh sách tính năng quan trọng: {str(e)}"
        )

def _get_feature_description(feature_name: str) -> str:
    descriptions = {
        'comment_ratio': 'Ratio of comment lines to total lines',
        'ast_indentation_consistency': 'Consistency of code indentation',
        'naming_generic_var_ratio': 'Ratio of generic variable names (i, j, temp, etc.)',
        'ast_if_statements_per_loc': 'Number of if statements per line of code',
        'ast_for_loops_per_loc': 'Number of for loops per line of code',
        'cyclomatic_complexity': 'Cyclomatic complexity of the code',
        'human_style_overall_score': 'Overall human-like style score',
        'naming_descriptive_var_ratio': 'Ratio of descriptive variable names',
        'ast_avg_variable_name_length': 'Average length of variable names',
        'blank_ratio': 'Ratio of blank lines to total lines',
        'spacing_spacing_issues_ratio': 'Ratio of spacing inconsistencies'
    }
    return descriptions.get(feature_name, f"Đặc trưng: {feature_name}")

@app.post("/api/analysis/gemini-analysis")
async def analyze_code_with_gemini(request: CodeAnalysisRequest):
    try:
        analysis_id = generate_analysis_id()
        timestamp = datetime.now().isoformat()
        
        print(f"🤖 Starting Gemini AI analysis for {request.filename}")
        
        feature_analysis = None
        if ANALYSIS_MODULES_AVAILABLE:
            try:
                features = advanced_extractor.extract_all_features(request.code, request.filename)
                if hasattr(features, 'to_dict'):
                    features_dict = features.to_dict()
                else:
                    features_dict = features
                
                feature_groups = create_feature_groups(features_dict)
                assessment = calculate_assessment(feature_groups, features_dict)
                
                feature_analysis = {
                    "feature_groups": feature_groups,
                    "assessment": assessment,
                    "raw_features": features_dict
                }
            except Exception as e:
                print(f"Phân tích thất bại: {e}")
        
        gemini_result = await gemini_analyzer.analyze_code(request.code, request.filename, request.language)
        print(f"✓ Gemini analysis completed: {gemini_result.get('success', False)}")
        
        code_info = CodeInfo(
            filename=request.filename,
            language=request.language,
            loc=len(request.code.splitlines()),
            file_size=calculate_file_size(request.code)
        )
        
        combined_assessment = None
        if feature_analysis and gemini_result.get('success'):
            feature_assessment = feature_analysis['assessment']
            gemini_analysis = gemini_result.get('ai_analysis', {})
            
            feature_score = feature_assessment.overall_score
            feature_confidence = feature_assessment.confidence
            
            gemini_confidence = gemini_analysis.get('confidence', 0.5)
            if gemini_analysis.get('prediction') == 'AI-generated':
                gemini_score = gemini_confidence
            elif gemini_analysis.get('prediction') == 'Human-written':
                gemini_score = 1.0 - gemini_confidence
            else:
                gemini_score = 0.5
            
            combined_score = (feature_score * 0.6) + (gemini_score * 0.4)
            combined_confidence = (feature_confidence * 0.6) + (gemini_confidence * 0.4)
            
            combined_indicators = feature_assessment.key_indicators.copy()
            if 'key_indicators' in gemini_analysis:
                combined_indicators.extend([f"AI: {ind}" for ind in gemini_analysis['key_indicators']])
            
            if combined_score < 0.3:
                summary = f"Combined analysis: Strong human patterns (Features: {feature_score:.1%}, AI: {gemini_score:.1%})"
            elif combined_score < 0.6:
                summary = f"Combined analysis: Mixed patterns (Features: {feature_score:.1%}, AI: {gemini_score:.1%})"
            else:
                summary = f"Combined analysis: Strong AI patterns (Features: {feature_score:.1%}, AI: {gemini_score:.1%})"
            
            combined_assessment = AssessmentResult(
                overall_score=round(combined_score, 3),
                confidence=round(combined_confidence, 3),
                key_indicators=combined_indicators[:8],  # Limit indicators
                summary=summary,
                baseline_summary=feature_assessment.baseline_summary
            )
        
        response = {
            "success": True,
            "analysis_id": analysis_id,
            "timestamp": timestamp,
            "analysis_type": "gemini_combined",
            "code_info": code_info.dict() if hasattr(code_info, 'dict') else {
                "filename": code_info.filename,
                "language": code_info.language,
                "loc": code_info.loc,
                "file_size": code_info.file_size
            },
            
            "feature_analysis": feature_analysis,
            
            "gemini_analysis": gemini_result,
            
            "combined_assessment": combined_assessment.dict() if combined_assessment and hasattr(combined_assessment, 'dict') else None,
            
            "summary": f"Gemini AI analysis completed. Gemini: {'Success' if gemini_result.get('success') else 'Failed'}, Features: {'Available' if feature_analysis else 'Unavailable'}"
        }
        
        return response
        
    except Exception as e:
        print(f"Lỗi phân tích Gemini: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail=f"Phân tích Gemini thất bại: {str(e)}"
        )

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )